Microsoft Build May 19-22, 2025 Session: BRK225 Speakers: Aidan Fitzpatrick, Barth�L�My Kiss, Carl Woodward, Jesse Clayton, John Rayfield, Luyan Zhang, Manish Sirdeshmukh, Ran Dubin, Ryan Demopoulos, Sudhir Tonse Udupa, Volker R�Lke, Xiaoxi Han

Ryan Demopoulos: Okay, good afternoon, everyone. Yeah. Thank you. Thank you. This is BRK225 Bring your own model to Windows using the Windows ML Runtime. And today we talk all about the Windows ML Runtime. We're going to dive into it, understand sort of like why we have been designing Windows ML and how it's put together. We're going to do a live coding demo. So we got lots of exciting content to cover here. And we have six different machines on here, which is continuing to fulfill my forever goal of making the KVM switch cry. So all right, let's jump in. I am Ryan Demopoulos. I'm a product manager that works on the Windows development platform team. Right now I work on Windows ML and AI technologies. In the past, I've worked on things like WinUI and WinApp SDK. I helped ship the -- one of those features as well. So some of you may recognize me from that. And with me today is my amazingly talented partner in crime, Xiaoxi.

Xiaoxi Han: Thanks, Ryan. Hey, everyone. I'm Xiaoxi Han, a software engineer working on Windows AI APIs and also the Windows ML. I've been at Microsoft for about eight years having worked on various projects including Windows Subsystem for Android, PC Game Pass, and a couple more. Today Ryan and I are very excited to not just tell you but also show you some significant improvements we've made to the AI story on Windows. Our focus today will be on Windows ML, which serves as a key component for many of the capabilities we're providing for you, the AI developers, on Windows.

Ryan Demopoulos: Awesome. Okay, let's jump right in. We got a full 60 minutes, so we'll get going. So today, right now, Windows powers the vast majority of desktop and laptop PCs all around the world. That alone is sometimes staggering to think about, at least when I think about it. And when you apply it to the revolution in AI that we are seeing all across the world, it really means that that AI revolution is going to unfold on Windows more than any other place. One of the greatest advantages, I guess, powers of the Windows ecosystem, is the incredible diversity of hardware that is available to all of your customers. Ranges all the way from $300 laptops all the way up to powerful $3,000 desktop PCs, bristling with performance and memory. And if you build your PCs like I like to build mine at home, you can go even above and beyond that. We know that when you're writing software for Windows, targeting that diversity of hardware can sometimes be a little bit challenging, and

that can be especially true when it comes to AI with all of the differences in silicon that are available. And so our goal as a development team and with Windows ML is to really help you do three key things. The first is to write code that scales across all of that diversity of hardware, make your life easier to go and write that code. The second is to make sure that you are maximizing the performance of every individual PC that is running on your customers' machines. And finally, just to make your lives easier with dependencies, whether it's trying to figure out what dependencies you need, procuring them, including them in your installers, updating the management, that whole type of thing, that's a lot of work, especially in a diverse ecosystem, and so we really want to help you with that. So with all of that in mind, we're going to do a very quick demo and make the KVM cry. All right. So this is a Surface Laptop 7. It is a Qualcomm-based device, and on this device we have a small app that we've written. It is called ResnetBuildDemo.exe because we are absolutely amazing at naming apps. And what ResNetBuildDemo.exe does is it takes an image in, and it inferences it against the ResNet50 model. This app is only about, all things considered, about 2 megabytes in size. I'm going to run it now. This isn't a heavy workload. It's going to put the workload onto the Qualcomm-based NPU, on this device. You're going to see the device very briefly spike. So let's go. Actually, let me just show you here. First, we should see what we are classifying. We're going to classify this cute little puppy. All right, what is this thing? We're about to find out. So let's run it. And you will want to take a look at the NPU spike. Here we go. So the app ran. You can see a little spike on the NPU. Very quick little bit of work. What this is doing is it's putting that AI workload and running it locally on this device, on the NPU hardware that's on it. And with almost 100% confidence, it believes that it is a golden retriever. I'm going to next switch to this device. This is an Intel-based laptop, a Surface device, and on this device, we have the very same app, same files, same size, same everything, same code. We are going to run the ResNetBuildDemo against this cute picture of the puppy. You'll see the NPU spike here. Every time we've run it, it's a fairly small spike, but there is a spike here. You can see it. There's a really light workload. But you can see a little spike there running on the NPU. And again, it's put that workload on this Lunar Lake NPU to go and determine that this is a golden retriever. I'm going to switch now to this AMD machine, and we are going to do the exact same thing, run this here on the AMD NPU, and you can see a spike there. It's putting the workload there. And finally, we will switch to this. And this does not have an NPU in it. This is a -- I'm seeing fuzzies here, and that probably means I really did make the KVM cry. Excellent. All right, so this is a Dell Alienware with a 4090 in it, and we're going to put the workload on this 4090. I'm not going to show you a spike on the 4090 because this is like a speed bump for 4090. And plus the 4090 is already underload drawing the very desktop pixels that we've got. So instead, what I'm going to do is I'm going to show you this app. It's going to activate the GPU engine. It's going to put a memory load on it. So you can watch that in Task Manager. Let's just run it here. And there it is. You can see the GPU engine is engaged. It's got a memory load. It completes, and then it's done. So what we've just shown you here across all four of these machines is the same app, same code, running on pretty diverse silicon. We've got three different NPUs. Each of them are from different manufacturers. They are running -- they have different chip designs,

and we're running on a GPU as well, which is like a totally different piece of hardware. And the thing that makes all of this possible is a new built-from-the-ground-up version of Windows ML, so we'll talk about that. Up until now, our approach has been to make DirectML your one-stop shop for local on-device AI inference. We've been listening to your feedback on DirectML, and we've also been building experience ourselves, building AI experiences into Windows, for example, with Click to Do. And we've dwelt upon your feedback. We've also sort of analyzed our own experiences here, and what we've come to the realization is that we need something in the platform that's faster, faster in two different ways. First, just wall clock faster, something that can strip away the layers and the abstractions between your app code and the silicon that is running on your customers' machines. But we also need the platform to be able to move at the speed of AI innovation when it comes to silicon. So we're seeing a flurry of activity in the NPU space where new chips are coming out. These chips are very different. They're designed very differently. And we really need the platform to be able to keep up to the extent where when new hardware hits the market, the platform is ready to go for your apps on Day 1. And after a lot of contemplation around that feedback and that experience, we've designed a new Windows ML centered around those goals. This new Windows ML is going to serve as the AI nucleus of the Windows AI Foundry. If you didn't get a chance to catch the excellent talk from Tucker and Deanne earlier today, you should definitely go back and check that out. They talked all about the Windows AI Foundry. You can think of this as like an umbrella term, instead of technologies to just help you do local on-device AI in your apps for Windows. And what this means is that if you are, for example, using Foundry Local to tap into the ready-to-use Foundry catalog models, or if you're using the built in to Windows AI APIs to go and do things like Laura for Five Silica or text intelligence or all of those types of things, you'll be using Windows ML potentially without even knowing it under the covers. But if you want to bring your own model, or if you want to take a really high degree of fine-grained control over how AI inference works on the device, then you can use Windows ML directly via its public APIs. This new Windows ML is built around the ONNX Runtime as its underlying tensor engine. If you're not familiar with the ONNX Runtime, it's a fast, open, mature inferencing runtime that I'm sure some of you in the room have either kicked the tires on, or maybe you're even using it in your apps today. And because it's built around the ONNX Runtime, it's also designed to work with ONNX models. The ONNX Model format is a open standard format that's designed to give really high inference performance. In many cases, we see 20% or better inference performance after we've converted from a source format into the ONNX format, and it's also designed to be just very convertible itself. And what this means for you is, for example, if you have a PyTorch model, if you've trained one, or if you've obtained one, you can convert it into ONNX format and then go and run that with Windows ML to go and run your local on-device workloads. Windows ML talks to silicon via execution providers. Execution providers are kind of like a translation layer between the runtime and the variety of silicon that the runtime sees on the device. We have taken the execution providers that we ourselves have developed for the various experience we've shipped in Windows like Click to Do. We've refreshed them in working in partnership with our

hardware partners. We've added new dedicated GPU execution providers to target the wide range of GPU hardware, and then we've combined that with ONNX's great support for CPU, and we've bundled all of that and wrapped it all up into the new Windows ML. So what this means, I know there's a lot of words, but what it means is if you bring an ONNX model, and you run it with Windows ML, it's going to scale across all this different type of hardware intelligently so that you can just not have to focus too much on it and really work, focus on your workload. Each type of processor is a first-class citizen with its own dedicated execution providers to make sure that things run well. Also, really importantly, you should take note of this. Windows ML will work on any Windows 11 PC. So this includes, obviously, the growing list. Yeah, exactly, working on them all. It includes all of the Copilot+ PCs that have been shipping in the market for some time, obviously, and those are Copilot+ machines with dedicated NPUs. But it goes beyond that. So if you have, for example, this guy that has a powerful discrete GPU, you can go and run Windows ML on this as well. In fact, you can even run Windows ML on PCs with only a CPU. Just make sure you keep the workload reasonable for CPU. And there's lots of AI workloads that are very reasonable on a CPU, so you can go and use Windows ML for that as well. Xiaoxi, why don't we talk a little bit about dependencies?

Xiaoxi Han: Sure, let me do this. All right. So let's discuss how Windows ML can simplify the task of obtaining, deploying, and managing app dependencies. We know you want to focus on your app code because that is the value you are trying to bring to your customers. But the status quo today is that you also need to grab an AI runtime out of the sea of complex options, and that runtime becomes part of your app, where the burden of maintaining it falls on you. These AI runtimes can also require additional addons for talking to a specific type of AI hardwares, such as the ONNX execution providers further bloating your app. And of course, you need to include the models themselves. It is quite normal to have multiple copies of the same model, where each copy is tuned to work specifically well on the specific type of processors for a hardware manufacturer. What this means is that you either have to deploy multiple copies of the same model to your customer's machine, which use up valuable disk space, or you have to write complex logic in your installer, trying to figure out which model would work in this current PC that app is being installed to. All of these is your responsibility to find, deploy, and manage. It's just the norm, and it sucks.

Ryan Demopoulos: It does suck.

Xiaoxi Han: Well, Windows ML makes all of this much simpler. To get started, all you need to do is to add a NuGet package to your app. This new Git package includes the Windows ML runtime headers, and it's going to set up dependencies between your app code and Windows ML runtime automatically. You just need to call our provided bootstrapper API from your app installer to install the runtime. And since the runtime is packaged as a store main package, that means we, Microsoft, handle servicing fixes for you, so you don't need to worry about updating the runtime anymore. And once your app is installed and initializes Windows ML, then we will scan the current hardware and download any execution

providers applicable for this device. This is great for a couple of reasons. First, you don't need to find your own execution providers, include them in your app, and maintain them. And also, since we only download the necessary execution providers for this specific PC, that means you don't need -- your downloadable dependencies are very, very few of them, and also the size of your app can stay as small as possible. You don't need to worry about what PC or app it's currently running on. All of that are handled by Win ML. And suppose you've trained a custom model and have multiple variants that work on AMD, Qualcomm, Intel, and Nvidia. If you upload your app to Microsoft Store and references these models as resource packs with metadata indicating which device or which hardware this model is targeting, then Win ML will make sure that only the correct version of the model is downloaded when your app is installed. This means you don't need to have complex logic in your app installer to determine which environment and hardware this app is running at, and you won't need to worry about having multiple copies of your model on your customer's machine. The goal of all of this is to really minimize the burden you have in managing app dependencies across Windows' wide array of hardware.

Ryan Demopoulos: All right, let's just briefly talk a bit about performance. Windows ML introduces the concept of device policies. Device policies are basically a way for you to describe what outcome you want when you run your AI workload with Windows ML. For example, if you set the max performance device policy, Windows ML will attempt to get the most power out of that machine and achieve the fastest wall clock speed possible. So if it sees a discrete graphics card, like in a machine like this, it will select it, and it will go and run that workload there as lightning fast as it can. Similarly, let's say you're in a different situation where you've got ambient AI running in the background. It's long-running AI. Maybe it's analyzing something the user is doing while that user is interacting with their software, and you don't want to chew up all the battery life. You can set the min overall power device policy, and Windows ML will try and respect that. It'll try and sip as little power as possible. If there's an NPU available, it will select that NPU and try and run it there. If you don't set a device policy at all, or if you set the default policy, then Windows ML will just run things on the CPU. The CPU gives the greatest range of capacity. Every device has a CPU. It runs across the most types of different models out there, and it's going to give the best accuracy inference across that wide range as possible. And finally, if you don't want to sort of use any of those, and you just know exactly what you want, I really want to run it on the GPU. I really want to run it on the NPU. You can do that as well. For example, you can say, prefer NPU, and Windows ML will just respect that. If it's got an NPU, it'll go and select that device and run it on there as well. Eventually, in the not-too-distant future, we also want to add what we call workload splitting. So you can have a single AI workload that is split across multiple different types of processors to get even greater performance and maximize what you can get out of the hardware that your app is running on. So when we do that, we'll work that into our device policy system. Okay, so we spent a lot of time talking about sort of the vision of what we're trying to do. We've talked about some design choices that we've made, how things are put together, some capabilities that it's got. I think a

natural question maybe on some people's minds is when can I try this? And the great news is that you can try this today. So I'm happy to announce, yes, yeah. So we're announcing Windows ML 2.0 Experimental 1. If you're familiar with sort of the nomenclature of like Win App SDK or how we name things, experimental release basically means it's not yet ready or meant for production apps. So please don't use it in your production apps. Instead, we're sharing early and open bits with you so that you can try them out, download them, give them a spin, integrate them into your apps, and let us know what's working for you and let us know what isn't working for you so that we can make sure that we hone the APIs, hone the vision, add the capabilities set that you need so that we can ship a stable GA later on this year. So with all of that said, Xiaoxi is now going to be very brave and fearless. She is going to take Experimental 1 for a spin with live code from the bottoms up. Go for it, Xiaoxi.

Xiaoxi Han: Thanks, Ryan. Well, before we do that, a little bit more talking. Let's go over the API surface of AI Fabric, not -- sorry, of the Windows ML first. Windows ML has two layers of API. The first is called the ML layer. You can think of this as the main APIs that you use in the new Windows -- new Windows ML. One set of APIs handles in the initialization of the runtime. It exposes an infrastructure object that you use to download all the applicable execution providers and also make sure the runtime is up to date. The initialization APIs are exposed in WinRT, but we've also provided flat C wrappers with managed projections as a convenience so you don't need to learn and write WinRT. Another set of APIs in this layer is the Generative AI APIs. These are designed to help you more easily write Generative AI loops with language models. And since we're built around the ONNX runtime, there is a runtime layer which exposes all of the standard ONNX runtime functions via flat C APIs with managed projections. This is great for two reasons. First, if you've already written app code against the ONNX Runtime, then it is very easy for you to port it to Windows ML because all the APIs that you're used to are already available here. And also, since we're exposing the full ONNX Runtime API surface, that means you can take very fine-grained control over how on-device inference happens, especially if you want to do things that aren't exposed via the ML layer yet. All right, so with all that in mind, let's see this in action. So can you put on -- yep.

Ryan Demopoulos: Got you.

Xiaoxi Han: All right. So this is the quick ResNetBuildDemo app that Ryan just showed, and today I'm going to show you how to write this from scratch. The first thing we always want to want to do is to get the model. You can obviously obtain open source model from PyTorch or Hugging Face and convert them to ONNX by yourself, but if you are using a model like ResNet, I highly recommend using the AI Toolkit VS Code Extension because we have just added a new functionality to it that makes this much easier. So this is AI Toolkit for VS Code Extension. I can see it in the Extension page. So we'll go to the "AI Toolkit" tab here. Under the models section, you'll see a new conversion tool in preview, and it is just released today. What this tool does is it provides a streamlined experience to get a model from Hugging Face, convert it to ONNX, optimize, and quantize it so it works really great with

Windows ML. The model that has been optimized via the model lab has very fast inference, small size, and also low power consumption. These are not that important if you are doing cloud AI, but on local AI, they are the key characters that you are absolutely searching for. So let's take a look at this. I'm going to create a new project, and here you see a list of 11 popular models, including ResNet. For these models, we have worked with our IHVs to really optimize them -- optimize them and make them work really well on Windows ML. But if you just want a model that is not in this list yet, you can still convert it to ONNX using Olive or other tools you like, and they should work with Win ML, as long as they work with ONNX today. They might not be as optimized as these ones yet, but this is just a preview, and we're still working closely with our IHVs to bring more models here. All right, so let's select "Microsoft/ResNet-50," and we're going to click "Next," and I'll save it in my default folder and name it resnet.

Ryan Demopoulos: Again, excellent name.

Xiaoxi Han: I mean, it's straightforward, right? So we'll go back to our AI Toolkit page and click on "Conversion" again. Here you'll see the workflow we just created. It has three options: convert to QNN, convert to AMD NPU, and also convert to Intel NPU. These are the three workflows that produce the models that are best optimized for the Qualcomm NPU, Intel NPU, and AMD NPU. We are working with them so that in the near future we have one recipe that will work on all of the three so we don't need three models anymore, and just one will work for all, but we're still working on that. All right, and since I'm demoing on the Surface Pro 7 with a Snapdragon NPU, I'm going to go with the "Convert to QNN" flow. I'm going to keep all the default parameters and just click "Run." Going to make this slightly smaller. Actually, no, I'll just scroll it. All right, you can see that we just click "Run," and it's converting a model to ONNX. In a machine like this, it's normally going to take about 30 seconds to a minute to convert it to ONNX. Assume that you have already installed all dependencies, so we won't spend our time waiting for that. But instead, I'm going to open Visual Studio and create our app. So I'm going to create a new console project. Of course, I'm going to name it ResNetBuildDemo.

Ryan Demopoulos: Yes.

Xiaoxi Han: All right, and going to keep all of that.

Ryan Demopoulos: Can't name it something different. It's the same app that we showed.

Xiaoxi Han: Yeah, yeah.

Ryan Demopoulos: It has to have the same name.

Xiaoxi Han: Just keeping our promise.

Ryan Demopoulos: Yeah.

Xiaoxi Han: All right, so the project's created. The first thing we want to do is to change the.net version, so I will open the project properties. So we'll change the Target OS to Windows, and the Target OS version will be 10.0.26100.0. The next thing we want to do is to install our NuGet package. So I'll bring up NuGet manager. We'll search for our NuGet, which is Microsoft.Windows.AI. MachineLearning. I'm going to install our version. Well, this NuGet contains the Win ML runtime package, which you need to install. I think I select wrong one, so I'm going to reselect. It contains the Windows ML runtime package, which you need to install from your app installer. It also contains ONNX Runtime bits and also the ML layer APIs that download and provision the execution provider package for you. Now that that's installed but before I move on, I want to show you this. If any of you here has worked with ONNX before, you will find these NuGets familiar. These are the execution provider NuGet package that you used to have to include in your app if you want to target these providers, and they're compiled to your app code, so they will be deployed to a customer's machine, which might not even support these. And that makes your size, app size, larger than you want, right? But with Windows ML, you no longer need any of those. We're just going to download them in runtime for you, and that's it. Let's go back to our install list. This is the only NuGet package we'll need for this demo. Now, if you were to build a app using large language model, you might need the Generative AI NuGet that we mentioned briefly earlier. It has a bunch of great helper methods that I won't be able to demo today, but we do have link to documentations at the end of the session, so please check it out later. All right, so we have prepared all the dependencies. Let's start write our app. I'm going to delete all that, oops, go back here. I'm going to bring up some namespace that's going to make my app looks better. I'm going to create a main class, sorry, a program class with a main method. This is the entry point of our console app. Now, the first thing we want to do is to initialize ONNX Runtime environment. This controls the logging level of ONNX, and this is a required thing by ONNX Runtime, so we'll do that. The next thing we want to do is to initialize Windows ML. We will create the infrastructure object which needs to stay alive for this entire app process. Then we call download packages async on the infrastructure object, which will scan this hardware and download missing and applicable execution providers packages. For example, for this machine, it will going to be downloading the Q and execution providers. And then we'll call register execution provider libraries async, which registers the EPs with Win ML so we know what to use when we try to deploy the AI workload. All right, that's all we need for initialization. And starting from here is going to be standard AI code writing. So we need to prepare a couple of path. Well, we need the path to our model file, which I don't have here because I'm hoping that AI toolkit has finished converting which, yeah, it did. So you can see it has the nice evaluation results. You can also run the inference sample in Python. But here I'm just going to copy model path from here and then paste it to here. Hope I didn't make any mistake. All right, we have a path to our label file, which contains 1,000 labels that ResNet used to classify images. And lastly, we have a image file that shows the puppy that Ryan classified before. But in case anybody forgot how it looks like, it looks like this. It's a ball of fur. All right, so I'm going to close that. All right, the next thing we want to do is to create the ONNX session. So let me

create the session. We will need a session options, which currently I'm keeping all -- everything as default, so it's going to run the workload in CPU. Now we create the inference session with the session options we just created. And now I need to load the image that I just show you. Here, I've written a couple of helper method to do that, so I am also going to bring that up using code snippet. So all it does here is to read an image from a file path into a software bitmap, and then it processed the software bitmap so it has the correct width and height, correct size, correct format, and correct normalization that's required by ResNet. These are standard image pre-processing steps. You can do it my way, or you can do it using helper library like OpenCV. So I won't go into too much details here, but all of these code will be uploaded to GitHub tonight, so you'll be able to check it out as well. All right, so the image is loaded. Now let's try to run the inference. So I'm going to do my inference helper, which put the image input we just load into a tensor, and we just feed that tensor into the -- or ONNX Runtime session and run the result -- run the inference. And finally, we want to show the output. So we get the results. We extract the output name and result tensor, and we just print it in a user-friendly way. So let me bring up my print results helper as well. Here we're just loading the labels into a list of strings, and then we -- before we try to print anything, we need to apply softmax to the results from ResNet because they're raw digits, and we need softmax to transform them into probabilities. And then we just sort these probabilities and print the top five class with the highest confidence. So let's try to run that. Hopefully it builds. Sounds like it did. And hey, you're seeing the same results that Ryan just showed before, yay.

Ryan Demopoulos: From scratch.

Xiaoxi Han: All right, so it still think it's a golden retriever. There's some chance that it could be a doormat, which I don't know about that, but --

Ryan Demopoulos: I'm going with tennis ball.

Xiaoxi Han: The chance is pretty low.

Ryan Demopoulos: Tennis ball looks good, pretty good.

Xiaoxi Han: Yep, yep. All right, so that's good. We just show this app running on CPU. Now, I mentioned that this machine has a MPU and a GPU, so in real life, you probably want to run your workloads on those devices. So let's see how to get a max performance out of this. I'm going to modify my session options, and all we need to do is to set an EP selection policy to max performance, and this is going to translate to using the GPU if the GPU is available. So let's try that. Oh, and before I do that, I need to add a sleep here so the app doesn't go away too fast. All right, so -- and let me also bring up Task Manager and make sure it's showing GPU correctly. And I don't need console. Need ResNetBuildDemo, all right? I think we have the correct setup, so let's try to run it. So here you see that it is actually engaging its GPU. So by just changing that one property, we moved our workload. Now, if I were to take this app to that powerful Alienware, it's going to pick the TensorRT RTX execution provider

instantly and going to load their workload there. All right, and what if we care about power consumption? So then we can change this to min overall power. I don't need that space. And this is going to shift the workload to NPU. So let's take a look. Let me close that. All right. So five, come on, all right. So here you can see a little spike on NPU. So we have moved the workload to NPU properly. And again, if I -- if I take this app right now and put it in my MD machine or my Intel machine, they're going to be able to use the Vitis or OpenVINO NPU execution provider properly. So we have really made this much easier, and you don't have to specify a device kind and anything like that in your session options. Thanks.

Ryan Demopoulos: Yeah, awesome.

Xiaoxi Han: And let's take a look at the output of our app. Here you see a bunch of outputs written by our execution providers. These are actually doing the model compilation. Now this ResNet is a small model, so it doesn't take a long time to compile. And also, I forgot to mention that ONNX, before you load any model to MP or GPU, ONNX have to compile it to this execution provider's specific requirement so they can be loaded properly. And if the model size is huge, like couple gigabytes, then the compilation step can take a few minutes, which you definitely don't want to see during every run, right? So we, to make this better, we introduced a model compilation API in ONNX, which can pre-compile the model, save it to disk, and you can just load the pre-compiled version next time when you want to run it. So let's see that. All we need to do is to change -- before we create a session, let me do this here. All I'm doing is I have a path that I want the compiled model to be, and if it's missing, I'm going to say it's not compiled yet. So I will create a ort model compilation options using the session options we just created. So this way, it's going to inherit the device policy -- device selection policy and use the correct EP. And then we set the input model path to the raw model and output path to the path I want it to be saved and just call compile model. It's pretty straightforward. All right, and the last thing I want to do is to switch the path I use to create a session to the compile path. And let's give it a go. And I forgot to kill this app, but you can see that the output is refreshed, and it says model compiled successfully. You see a same inference results. And now, if we were to run this app again, you'll see that all of those are gone. It's using the compiled version. It's slightly faster, but on ResNet, it's not very noticeable. But again, if you're using large language model or something like that, this is going to make your life much, much better. And I think that is it for my demo. I hope we find it useful. And again, all the code will be uploaded GitHub tonight, so you'll be able to try it out. We will have link to documentations for all of our APIs, and we'll have the link posted at the end of the session, so you'll be able to check it out -- check that out as well.

Ryan Demopoulos: Awesome.

[

That was amazing. You wrote that all the way from scratch. That's so good. Awesome. Okay, so just before we move on from that demo, I'll just add one other point. So the code that Xiaoxi wrote or any code that you write against Windows ML, it won't just work on this hardware up here today or even just the hardware that's in market right now. It'll actually work on future hardware that comes to the market later or even hardware that has never been designed or conceived yet. Earlier, Xiaoxi mentioned that Windows ML will always scan, oh, shoot, thank you, Xiaoxi. Yeah, I think we're good here. Earlier, Xiaoxi mentioned that Windows ML will always download the execution providers for the hardware that it sees on whatever device that it's running on. What makes that possible is a deep level of collaboration that we've done and a new certification program that we've stood up with all of our major IHVs across Nvidia, AMD, Intel, and Qualcomm. The way that this works is that as these manufacturers bring new hardware to market, at the same time, they will either refresh or create new execution providers to coincide with that new hardware. They will submit those new execution providers to us, Microsoft. We will test those execution providers, certify them to make sure that there aren't regressions and inference accuracy, make sure everything looks good, and then we'll put them on our servers and make them available to Windows ML to be able to go and download when it sees new hardware on the device. You don't have to change any of your app code. So basically, you write to Windows ML. There's future devices hit the market, and it just works, and you didn't have to go and ship some sort of an update. That's effectively the vision of the certification program in this collaboration. These hardware partners have just been fantastic to work with. They've really bought into the vision. They understand that the power of the Windows ecosystem is all this hardware diversity, and we have to make it so that all of you who are writing software can target all of that hardware. They've been fantastic. You've heard me talk a lot about what we're doing here. I want to just take 60 seconds so that you can hear it in their words. Let's take a listen.

Manish Sirdeshmukh: In the PC ecosystem, as we all know, there are multiple IHV platforms with their own architectures and SDKs. What the ISVs need is a way to develop and deploy apps such that they are platform independent. The Windows ML runtime offers exactly that by auto install of the runtime and execution providers specific to the hardware it's running on. ISVs do not need to select EPs at compile time.

Jesse Clayton: Windows ML helps Nvidia and other hardware vendors deliver optimizations to ISVs and users, while reducing the barriers to adoption. It empowers vendors to productize software innovations faster and allows app developers to unlock interesting new use cases.

Sudhir Tonse Udupa: Windows ML serviceability infrastructure will automatically update the inference software engine of AI so that your applications can adapt as the hardware capabilities emerge.

John Rayfield: Windows ML is going to be a standardized way of landing workloads on these platforms across different engines, so be it NPU, GPU, or CPU, through leveraging these deliverables, which through the quality certification process with Microsoft, your ISVs and customers know that this workload is going to land on the right place and the right accelerator with the right balance of performance and battery life.

Ryan Demopoulos: So as I look out in the audience, I see a few of you IHVs out there who've we've been working with. It's been a real pleasure meeting with you. Pleasure and a lot of work meeting with you multiple times a week in person, over Teams, trying to go and send this up. Thank you so much for that.

[ Applause ]

It's not just our IHV partners that we've been partnering with. We've also been very busy partnering with a number of development partners as well. The logos that you see on the screen, all of these folks, we've been sharing early builds of Windows ML, getting feedback, helping to hone the APIs, helping to hone the vision, finding out what works for them and what doesn't as they take it and prototype and integrate it into their apps. And similarly to how you can sort of hear it in the words of our great hardware vendors, I think it would be worthwhile listening to what these folks have to say about Experimental 1 and their experiences trying it out. So let's take a quick listen.

Volker R�Lke: Premier Pro and After Effects are leading professional video editing applications that often handle terabytes of video footage. Our goal is to adopt the new Windows ML once it matures enough to handle the heavy ML workloads required by our video editing apps. A reliable API that ensures consistent performance and accuracy across heterogeneous compute devices would remove significant obstacles.

Carl Woodward: We look forward to replacing model load and inference code from multiple SDKs with Windows ML. This will simplify our code and testing whenever runtimes update. And we're also very excited that Windows ML could truly deliver build once work anywhere models, and we can't wait to verify and provide feedback on this possibility.

Ran Dubin: Windows ML is built upon the specialized version of ONNX Runtime for ISVs like Bufferzone, which focus on Windows devices. This is significant, since research teams can train AI models with Python environment that contains various packages and dependencies while ONNX enable us to forget about those Python dependencies and use.NET and C++ easily without the complexities and Python dependencies.

Luyan Zhang: The simplicity amazed me. Following Microsoft easy approach, get an ONNX model, add it to your app, and integrate it into your code, we converted a complex AI feature to Windows ML in just three days.

Aidan Fitzpatrick: We've already adopted Windows ML to help us prototype it on a number of silicon platforms, and it's going well. It wasn't a big lift from ORT, and it gave us the accuracy and processing performance that we expected.

Barth�L�My Kiss: Powder is an early adopter of Windows ML, and it has enabled us to integrate models three times faster, transforming speed into a key strategic advantage.

Ryan Demopoulos: Okay, so earlier, we saw Xiaoxi's demo. Was a fairly simple demo. Was a console app. Why don't we take a look at something a little bit more sophisticated using Experimental 1? If you could all please join me in welcoming Bart, CO and cofounder of Powder.

[ Applause ]

Should be good.

Barth�L�My Kiss: Should be good. We are Powder. We run AGI tools for gamers, powered by Windows ML. What is Powder? We transform gameplay into highlights automatically. How does it work? You just play. We record, find great moments, and package them for sharing. Under the hood, we use Windows ML to keep things smooth across hardware. It's the kind of AI-native experience that just starts to be possible. So this is Powder running on a Windows 11 Copilot+ PC. It's an Asus Z30 powered by an AMD Strix Halo. I'm going to start a recording, and we're going to have an AI model running live to analyze gameplay. Let me check the game first. Okay. So this is a quirky indie game created by Tom, who is in the audience.

Ryan Demopoulos: Hey, Tom.

Barth�L�My Kiss: One of our developer. And the AI model has been trained to detect level completion, which I just made one of. I'm going to do another one. In the background, there is a vision model that is running to detect events. All right, enough digging. I'm going to finish the recording. So let's see what Powder found. Okay, so we can see a clip with two events in it. Level completed once, twice. Perfect. That's exactly what we were shooting for. For game developers, there is no SDK, no integration. We strictly build on top of our semantic understanding of the audio and the visuals of the game. And if you're a developer, we didn't deploy this AI model for one NPU in particular. We use Windows ML, and it enables us to deploy it across silicon. We do the work once, and it works everywhere. Now, let's --

Ryan Demopoulos: Yeah.

Barth�L�My Kiss: Thank you, and thanks to the Win ML team. Let's switch to something a little bit more cinematic perhaps. It's a game called Skull and Bones from Ubisoft. You can see about 40 minutes of gameplay. And in it, we found a number of player eliminations, of ships being sunk, really. And all of that was detected live on the -- on the NPU. And if you

like to create great videos, but you don't like the hassle of video editing, the Powder auto montage is where it all comes together. Let me show you.

[ Music ]

Powder is like memories, but for PC gamers. We take the raw footage and turn it to great stories that you can then share with your friends and your community on YouTube, X, Tiktok, whatever fits your world. Gamers get great footage, great content. Publishers get great UGC, everyone wins. And the best part? This wasn't a heavy lift at all. We are a small team of 15 people, 10 developers, and the promise of Win ML will help us achieve our goal of deploying our AI model across all silicon. If you're a developer, this is your platform where it helps you deploy serious AI on device without needing deep silicon knowledge. We're proud to be early, but we're 10 times more excited to see what you create with it. Thank you.

[ Cheers ]

Ryan Demopoulos: Bart, that's awesome. Thank you so much. It's been -- it's just been a real pleasure working with Powder. Thanks, guys. It's been a real pleasure working with you. This is on Experimental 1. We can't wait to see you guys ship this and other features once we hit stable. It's going to be great.

Xiaoxi Han: Yep. Thanks, Bart.

Ryan Demopoulos: Yeah.

Xiaoxi Han: I mean, I'm a big gamer, myself. I'm not really good at it, but I think I really need this to catch all of the precious moment I'm like really good at and then show it off to my friends. So I can't wait to see this in market and using our great Windows ML runtime. All right, so all of that are super exciting. And what is even more exciting is about our release roadmap. So as you've seen, we've released Windows ML Experimental 1 today. It already contains a lot of the capabilities that we've discussed today, including support for all types of processors, a auto execution -- auto EP provisioning, so download the execution providers for you. And an early version of the auto device policy for selecting -- directing AI workloads. And it also includes a full honest runtime API surface to make it easier for you to pour your app into Windows ML. Note that this is an experimental release, meaning you should not use it against production yet. Please, definitely try it out. Your feedback is very valuable to us, but we strongly recommend against using it -- shipping an app on top of it. All right, and later this year, we will ship Windows ML 2.0 Stable. The exact date is going to depend on the stability of our bits and also the feedback we received from Experimental 1. This will be the first launch, the first fully support launch, and that will be ready for production use. It will have a lot of improvements, especially based on your feedback today. And yeah, we really hope that you found this useful, and it's going to help you to

make a great AI experience on Windows. Right, with that, I'm going to wrap up my talk today, our talk today. Again, please, sorry, Ryan.

Ryan Demopoulos: That's OK. I'm here too, but, all right, I'm out.

Xiaoxi Han: Come back, please. All right, please try it out. We have a aka.ms/TryWinML link that points to our documentation. Again, all of the code to our sample app that I showed -- I wrote today. And also there will be another sample apps in Python, and also C++ will be uploaded to the aka.ms/WinMLBuildRepo link tonight. So it's not ready yet because I need to clean up -- clean up my code. And also, if you want to talk us -- talk to us later, you could find us in The Hub. There will be a experts meetup booth. Ryan sometimes will be there. I will be there. And a lot of other experts in Windows AI will be there, so come and chat with us. And if you want to file feedback, which you definitely should, the first thing you can do is to, once our GitHub repo is live, you can file GitHub issues from there. If you have general feedback about Windows AI Foundry, you could email Windows AI at info@microsoft.com. And of course, you can also use feedback hub to file feedback. And also, these are some of the related build session informations. There is a great overview of Windows AI Foundry that happened earlier today. Also a more detailed talk about AI Foundry that also already happened. So if you've missed them, they are recorded, so please check out the recording. And tomorrow, from 3:00 to 4:00 at this room, there will be another talk about Windows AI APIs. So if you are interested, please check it out, and that's it. I hope you enjoy our session.

Ryan Demopoulos: Yeah, thanks very much.

Xiaoxi Han: Thanks.

Ryan Demopoulos: Hope it was helpful to you. Thank you.

[ Applause ]

END