Microsoft Build May 19-22, 2025 Session: BRK104 Speakers: Brady Gaster, Ed Charbeneau, Jeremy Likness, Jon Galloway

Jon Galloway:

[ Music ]

Let's go.

Jeremy Likness: All right, congratulations and thank you. It is early in the morning, and you've been conferencing quite a bit, so we appreciate you coming out. This is Building the Next Generation of Apps with AI and.NET. So, let's get started. We know that things are moving at a rapid pace. In 2022, which was just a few years ago, ChatGPT was announced, and that's what brought generative AI to the general masses, and pretty much everyone is using it today. In fact, there's a very interesting statistic in that it only took them five days to get to 100 million users, which was unprecedented at the time. Now, if you take the length of a task that someone would ordinarily do in software, let's say it takes 10 minutes to do, and you ask a LLM, a large language, model, to do it with 50% accuracy, that length that it can accomplish is doubling every seven months, which means it's just an exponential pace for that. And the things that you can with generative AI seem endless. We used to have very specialized libraries. I'm sure some of you have worked with email .NET, and some of the other ways of doing classification, summarization, sentiment analysis, and all of these can be accomplished through generative AI. But what we're here today to share with you is the fact that all of these things can be done in .NET, and we're going to show you the ways and what we've done to make that possible for you. My name is Jeremy Likness, I'm a principal product manager, and I'm responsible for your experience working with AI and.NET, so if you haven't figured out the secret for Microsoft emails, it's firstname.lastname@microsoft, feel free to send me your feedback. I know I'm opening a can of worms there, but I'm here for you. I'm also joined by my friends, Jon Galloway and Brady Gaster, who will be sharing some unique demos with you (applause). Generative AI applications are being built on .NET, and they are in production. So, our own teams are doing this. All of your Copilot experiences, whether it's the Microsoft Copilot, GitHub Copilot, assisted coding, and Xbox Copilot for gaming, all of these are built on.NET. We also have customers, for example, H&amp;R Tax Block, who are using AI to enhance their applications and create a better experience for their customers. Our ecosystem is also thriving, so regardless of what type of component you need, if you're plugging into a vector database, or if you're working with a provider and managing models, we have companies that are building with .NET, providing SDKs with.NET, and considering it as a first-class platform for their products. This is a

summary of some of the investments we've put into AI and.NET over the past year. So, our foundational building blocks just went general availability, and that is the Microsoft extensions for AI and for vector data. And what these provide is a set of building blocks for you to interface with AI, and for the ecosystem to build solutions on AI. And we'll dive into that a little bit more. We also built a C#-based,.NET-based MCP server, so that you could take advantage of model context protocol, which is really -- I think of it as OpenAI, but for agents, so that they can discover capabilities and tools and ways to interact with each other. We've invested in AI templates, and so we have a getting started experience that does more than just, "Hello, World," but actually has some complex scenarios baked in, like setting up embeddings of vector database, bringing in other documentation to enhance your agent, and we'll take a look at that. Is there anyone using Semantic Kernel today? Just curious, show of hands. So, quite a few of you are using that, so they responded very quickly to the AI wave, and they have a multi-platform, multi-language solution, of which.NET is a first-class part of that. And then finally, we've got our model evaluations, which I'll talk more about, but it's a suite of tools that allow you to get scoring and dimensionality around your models, and by dimensions I mean things like the safety of output, or how grounded it is or how consistent or how complete. Our building blocks are really the key investment that we've focused a lot of energy on, because this enables a lot of scenarios. We basically took what was very successful in ASP.NET, and if you're a web developer you're familiar with things like middleware and dependency injection. And we wanted to provide that environment in AI, so that we could have that extensibility, that interoperability, and give developers a consistent interface to talk to when they're building their AI applications. And I want to showcase one of our partners who took advantage of this, and if you've done anything with Blazor, you're probably familiar with Telerik, who provides a suite of controls for Blazor. And I'm going to let Ed explain how these building blocks help them.

Ed Charbeneau: Because Microsoft.Extensions.AI has given us the IChatClient interface, and this is something that we can rely on in the.NET code base, we've enabled it on our Telerik AI Prompt component out of the box. And what that means is you can start with a.NET Blazor or ASP.NET application, and use the IChatClient service in your project, and that will automatically light up the UI within our Telerik AI Prompt component. All we have to do is first register our IChatClient in the program CS file, which I already have here. Next, all I have to do is add the AI prompt to my application, so I'll say, "Telerik AI Prompt." When I save this, my project will update, and it can immediately start chatting with my AI assistant. I can say, "What is Telerik UI for Blazor?" hit "generate", and this will use the underlying IChatClient to generate the text for me. This one's been back-ended by the OpenAI services in Azure. Now, I can even extend this further by taking the AI prompt, and doing a little bit of customization with it, I can set up some prompts that are pre-determined to help guide the user, so when they're using the component, there are pre-determined values for them to just click on and generate some text. So, we'll go ahead and refresh the application, and you can see that I've got some prompt suggestions here. These are acting as a marketing

role, and I can just click on one, hit "generate" and I have my AI-generated output. With just a few lines of code, you can have the generate AI solutions from large language models baked right into your Blazor and ASP.NET applications. If you'd like to learn more about how we're integrating our AI chatbot with other UI components, visit demos.telerik.com, and look for the Blazor Labs tab on the left-hand side.

Jeremy Likness: Okay, so that was based on our building block primitives, and you have access to those through a variety of ways. For example, if you've worked with a Llama, which is open-source models, a LLamaSharp, the library that works with it, comes with these built in, so you can just use the IChatClient anywhere that you want to, and it's injected into the dependency injection stack for you. But what I want to do now is talk a little bit more about the templates that we've worked on, and invite Jon to give you a walk-through on how to get started with AI in.NET.

Jon Galloway: Alright, couldn't recognize me, good way to start, and boom, and I'm on number three. Good morning, folks. Okay, I am so excited to show this off. This is some of the most fun I've had coding in a while. One of your big homework assignments, you've probably seen tons of AI stuff all week, Jeremy showed you some neat demos already, you're going to see more later, the quick getting started experience. So, if I say ".NET new" and -- so it's ".NET new install", and let's blow that up, okay, so that installs the template. That's all you got to do to get that going. Then I pop over into Visual Studio and I create a new project, right? So, I'm going to use this AI chat -- web chat app, so now, if I go and look at this, this is going to say, "What do you want to call it?" This is where the magic starts happening in just a second. Okay, so if we go and take a look here, here are my options for getting started quickly. So, as I'm developing with AI, I've got a range of scenarios. I'm starting locally, and I just want to start a project. I just want to get going. I don't want to pay any money, right? So, if I've got a beefy computer that I'm developing on, spoiler alert, I don't today, so I'm not using this option, but I would use a Llama, right? So, a Llama uses your local processor, gets -- it takes advantage of your GPU, all that stuff. Thanks to the magic of Aspire, it actually is pulling that down, sets it all up for you, so that is super-cool. Another thing, which is what I will be doing in just a second, is using GitHub Models. GitHub Models is free. All you need is a GitHub account, which hopefully you all have, and you just create a token and pop it in there and party, okay. That is awesome for development, but you're capped. You get a certain number of tokens for free. Makes sense. So, this is amazing for local development, for prototyping, for getting started, working against, like Ed showed you, that IChatClient. So, you write all this code, then you're ready for production. You've got a few options here. You've got Azure OpenAI, it's amazing. This works, of course, with all the Azure ecosystem and infrastructure that you know and love. It's got all the security, it's got all the -- baked in with Azure, so that is great, or if you want to work against the OpenAI platform directly, IChatClient, keep on partying, it's all the same thing, right? So, that is cool there. I'm going to pick GitHub Models. Then I'm going to make sure I click "Aspire orchestration", because I definitely want that. I'll show you why in a second. Now,

you've seen tons of demos that show you how to use -- to connect to an AI service provider, which is, of course, like, you need that, right? That's your large language model running in the cloud or on your computer. But one of the things that you may not have heard talked about enough is a vector store, and that is where a lot of magic happens. You'll see in this application we're going to be processing PDF files locally on disk, and it's actually going to be creating embeddings and putting those in a vector database. Super-powerful, sounds a little bit complicated, but fortunately, this is all pre-wired for that, okay. So, I have a few options there. I could use local on disk for prototyping. Not recommended. It's an option, but it's just JSON files on your disk, low performance and no real vector database goodness. When I deploy to production, I might likely use Azure AI Search, so this is built on Azure AI Search. It has all the Azure, again, integration and stuff, scale, amazing stuff there. Also, Qdrant, Qdrant is an option, both for local development and for deployment. So, Qdrant is -- I call it -- I don't know how to pronounce it, is it Quadrant?

Jeremy Likness: Qdrant.

Jon Galloway: Qdrant, so it is super-cool. Okay, so it is a vector --

Jeremy Likness: Q is silent, Drant, Drante.

Jon Galloway: So, this database, which I'm not going to pronounce, which shall not be named, so Qdrant is really cool. It is a -- it's going to be pulled down for me as a containerized thing via Aspire, and let's just watch it happen, okay, so let's zoom out, because I'm getting dizzy. And we're -- I made sure I have checked "use Aspire orchestration", here we go. So, the first thing it's going to do is pop up a README. Now, if you're like me, and I know I am, I immediately close all READMEs that pop up, but don't do that, because this actually has some really important instructions. And so this one has some stuff in there about, "Hey, you need a GitHub -- you need a token to use GitHub Models," okay. It tells you how to set it up and all that stuff. So, I'm going to pop over here. I've already ready this a thousand times, so I'm going to go into my app host, I'm going to do "manage user secrets", so this is where I'm going to pop in my connection string. So, let's make sure I am zoomed up enough that you can't read the whole thing. Alright, so there we go, so that is my connection string, and now I should be ready, unless I've used up all my tokens today, which I was -- maybe, let's see. So, now I'm going to make sure Docker s running, and we'll just run it, and I'll show you what's going on. Docker is running, okay, so let us just run this.

Jeremy Likness: Mental note.

Brady Gaster:

[inaudible]

Jon Galloway: I always get thrown off by working on a different resolution. Let's see how we do. So, this is going -- if you're not familiar with Aspire by now, this is the magic of pulling all the stuff together, orchestrating it, running it all, launching it with the app host, and then showing me the beautiful dashboard. This is the newest release that just came out Tuesday, and it includes some new treats in there like the Qdrant dashboard is shown right in the Aspire -- or the Qdrant dashboard is linked from the Aspire dashboard, which will make sense in a second. Okay, it's all running. That was fast. If we go in here and look at the console logs, it has actually ingested some PDFs, okay, so what that means is it's read the PDFs, it has extracted the text, and it's thrown that in the vector database. So, as Jeremy said, this is not just a toy, this is an actual working app that does some neat stuff. So, I'm going to go over here, take a look, I'm going to say "attempt". Whoa, let's not do that. I don't know what it would respond with. Okay, so I'm asking about a tent, and one of the PDFs that I've ingested has information about that. I click on it, boom, and then it takes me directly to that line of code. So, I'll show you really briefly how that magic happens. That is actually -- there's a few things that are set up in here. There is in the Solution Explorer, we'll go to the app host, and that configures -- it pulls in, as Jeremy said, all the familiar things you're used to, right? So, it provisions the resource using VAR AI equals builder, blah, blah, blah, creates the vector database, so it's pulling in as Qdrant, and then passes that stuff over to my web app. In my web app, we configure a few things. Now, the magical thing here is that this VAR OpenAI, it's injected, it's a service I can use throughout the app, and it is an IChatClient. So, what that means is, later on I want to switch my -- I want to switch to Azure OpenAI, I don't change any of my code. I just change what's in my app host, and I say, "Now point to Azure OpenAI," right? So, we actually create two different AI researchers. We create GPT-4o mini, which you're used to, those LM -- LLM things. If you're not used to embeddings, this is where we ingest PDFs into, and it stores that information as vector data. So, let's look at that vector data, and that is one of the things in here which is a -- if we go to an ingested chunk, I believe, has it, you'll see these annotations for things like vector store key, vector data as index, blah, blah, blah. So, these are standard.NET annotations. Microsoft extensions for vector data know how to process this, and know how to map that, so you don't have to do all this work to figure out, "How -- what do I -- how do I turn a chunk into vector data?" blah, blah, blah. It handles all that for you. Don't have time to go into this in depth, but there's a great walk-through and there's documentation on how that works. What I want to do is just show really quickly, I'm going to extend this very slightly, I'm going into -- there are two PDFs that are here. I'm going to add on just a couple more. So, I'm going to say, "Add new existing item," I'm going to go to my downloads, and my downloads is a mess, but fortunately, I've thrown these into a separate folder, and we'll say "add", okay. So, now I've thrown in two PDFs with information about build, so let's go ahead and run that, so I'll do "debug", "start debugging". So, now I'm just going to re-run this, and I'll show you two more little things. Ill show you that number one -- oh, what happened there? Maybe I didn't stop it from running before. I'm not going to mess with this. Well, you saw it running once, so I'm going to count this still as a demo win. The one thing that I was going to show is that it actually allows me to search through that build data as well, and it actually -- I put in

the map of build and I said, "Where is the food?" and it actually jumps right to that too, which is amazing. The other neat thing, just to wrap up, is this IChatClient, like I mentioned. That is the magic of how everything all ties together, and that actually, if I go into this chat, if I go to chat.razor, you'll see right here this chat client, and that is what's tying it all together, and then here's your simple query. So, if -- I want to make sure you get a chance to see all the other deeper-dive stuff. If all you take away is file new project, AI web chat app, and start playing, this is the best way. You start playing for free with GitHub Models, and then you can scale up, and this is production-ready code that can then deploy to production on Azure. Thanks a bunch.

Jeremy Likness: Thank you.

[ Applause ]

In two, four, one, one. Perfect, thank you so much. So, we've talked about this again and again, but these extensions are basic building blocks and primitives, and we call them "primitive", but they're very complex in what they can do and accomplish for you. So, let's talk about everyone's favorite topic in AI, which is agents. We love agents, so you've probably heard a lot of different definitions and terminology. I'm going to give you our take on agents, and that is really large language models that are enhanced by different features and services. So, we can give them tools, so that they're able to access real-time data, be augmented with other data that you have. We can give them memories, so that if there's a long-running conversation, for example, that can be provided as a service. There might be data that augments the agent. And then you have things like orchestration of agents, so how do we determine what agents we have in our environment? Where are they? How do we route to an agent? And finally, workflows, in which agents actually might make decisions and help route something through a long-running business process. So, another way to think about agents is really AI designed to perform a task. So, the same way we took business systems and broke them down into microservices, agents are ways of breaking down AI into interfaces, if you will, so each agent performs a specific task. And that might be a very simple task, which I use "simple" lightly, because summarization and classification and sentiment analysis has a lot behind it. But that might be a task, or it may be something more complex like orchestration and workflow automation. So, our building blocks are agent-ready. I get a lot of questions of, like, "What is Microsoft extensions for AI doing for agents?" And the bottom line is this is an example of using Semantic Kernel, and here I've got a builder, so what I'm doing is building up my chat client, and I'm using the IChatClient interface, which is what we provide as a primitive. And then Semantic Kernel will take that and allow me to parse that in and use that. So, we'll get into more specific code and discussion around this in a minute, but the bottom line is that there's things you do around agents to manage agents, and then there's the way you interface with agents. And we believe that the extensions for AI is the way that you'll talk to the agent when you have access to it, but you'll use other products like what's built into.NET with Orleans, for example, if you have a state for workflow, or maybe you'll use Process with Semantic

Kernel, and the right tool for the right job. This is just a snapshot of our ecosystem of offerings that address agentic apps. I'm not going to spend too much time here, because I want to dive into code and get out of the slides and show you something a little more. So, when we talk about the real world, what we're saying is we've shown you a getting started template, we've shown you some snippets, but we wanted to build a sample for you that has a lot of moving pieces and parts that looks a lot more like what your business systems are that are in production. We know they're distributed, scalable systems, and so we built a demo that addresses that, and allows you to see something a little more complex than just a conversation with a chatbot. So, I'm not going to ask you to go through this sequence diagram, but what the solution is is a travel booking and expense reporting. So, I'm sure many of you went through the whole process of finding an itinerary and booking to come to the conference, and then afterwards is everyone's favorite part of traveling, right? It's gathering up your receipts, figuring out what's reimbursable, and submitting your expense reports and waiting for reimbursements. So, what we want to do is take a look at how can agents help with that process, and what can we build to do that? So, this application has an architecture with a React front end, it has a.NET back end, but I also want to point out the Python logo in the lower right, so there is an agent that is running in Python. And the reason why I'm sharing this here is because a lot of times developers ask if it's either/or. "Do we have to go entirely into Python? Can we do it with.NET?" And our goal is to make everything you can do in AI possible through.NET, but also not force you to have to migrate existing code, or not work with existing services that just work. So, if you want to host Python in your environment, with.NET Aspire, that's possible, and I'll show you a little bit more about how that works. So, this is a quick walk-through of the demo, and to compensate for latency and unexpected hiccups, we're going to go through the smooth recorded one, but I am going to jump into live code in just a second. So, what we're doing here is saying, "Hey, look, I need to travel to Build, for Seattle, and I want it for tonight," and so the agent's going to go out and say, "Okay, let me figure out what the user's asking for, figure out, "Oh, they want to plan a trip, " and now it's actually going to book the itinerary for the trip. So, we got the itinerary, they're going to select it, but we don't want to just trust that the agent got everything right. We still want to have admin approval for this, and so the admin could go in and basically take a look. They say, "Looks good to me," and then they approve that, and then when we go back to My Trips, we're going to get the trip request approved. And then the next thing is the fun part of uploading receipts, where actually first what I'm going to do is ask, "What is reimbursable? Which receipts can I submit?" So, because this has been augmented with our company policy, it's able to give us the correct response for that. Now we upload the receipts, and what you're going to see is that it's going to come back and say, "They were processed, I looked at them, they look fine," and then finally we're going to say, "Let's get an expense report built from these receipts." So, "generate expense report", and it's generating the reply. And there's the expense report, and you see it's categorized the receipts, it's actually found the amount on the receipt, it's given a description, and this is a live demo that you can pull down, build and run yourself to see how that works. So, let's take a walk on the code side now, and see what we can see.

Jon Galloway: Can we also deploy that in the office so we don't have to do it manually?

Jeremy Likness: Yeah, right.

Jon Galloway: That would be great.

Jeremy Likness: Okay, I'm going to four. Okay, so in this section, the first thing I want to show you is something that we call structured data. So, you don't have to just get text back from the agent, you can actually get a structured data response, and it actually serves multiple purposes. So, one that I think most people have figured out is if you want to programmatically parse the response from an agent, getting it in structured data makes that easier. But the other thing I don't think people think about as much is that it also tells the intent of what you're doing to the agent, which is really important. So, here we have user intent, and if I look at user intent, which is going to be up in my domain project, and I'll come down here to "user intent", it's just an enumeration of possible values. But what we're doing in this code is we're actually asking it to take the user's message that they typed in, and then we're saying, "All you can give us back, Agent, is user intent, so parse this message, and tell us what the user's actually intending to do." Now, I'll walk through what that leads to in a second but I just wanted to point out that if you've worked directly with any model provider, there are different ways to handle structured data and it typically involves coming with JSON schema and publishing that schema. This is all done behind the scenes for you with our extensions for AI. So, if you give a structured class, just by using this extension method, there's a generic extension method. It will parse that class and provide all of the meta data that whichever provider needs in order to give that response. So, now let's take a look at what John was showing, which was the ingestion service. So, we have one of these here so we can ingest our travel policies and here you can see in my program, where I'm setting everything up, I have this new ingestion service. So, if we come over here and we look at our services layer, and we come in to our ingestion service, this will basically reference our vector data extensions. And what's that going to do is it's going to give me the ability to do something like this where I'm basically parsing my PDF documents and passing this structured document. And then it's handling everything else behind the scenes as far as turning it into embeddings, which are numeric data that represents the text that the large-language model can understand, and it's also storing them in the vector database behind the scenes for that. What's even more interesting, in my opinion, is if we look at the other side of this, when the user enters something that references that documentation, like, "What is the reimbursement policy?" this is going to use what's called semantic search. So, what it's doing is it's finding the text that most closely matches what the user entered, and then returning that portion as a response. This is what I have to write as a developer to make that work, so it makes it pretty easy. Now, behind the scenes, this is actually vectorizing, or actually turning your query into embeddings, sending those to the vector database, running a query, getting embeddings back, mapping them back to text, but all we have to do with our extensions is just iterate from the results of the search. Moving on, I'm going to go into the multi-modal aspect of this. So, let's say you do want to upload a

receipt. How difficult is that? Well, the way that the extensions are built is they have different content types, so there's a text content and there's overrides and extensions that allow you to just send text directly and behind the scenes, it'll wrap it in text content, but let's say I want to send receipts and a text prompt. It's as easy as either reading the bytes of the image and taking that blob of bytes and passing it in a data content, or taking a data-encoded URI and passing that in. And, in fact, if your provider requires a data URI and you pass bytes, the extensions will automatically convert that for you so you don't have to worry about that. So, this is simply taking exactly what they attached and wrapping it in this data content, or URI content, and then passing it to the model. So, moving along, now this is where I think it gets pretty interesting. So, this is our receipt data, and what I've done is I've created a record that tells the model exactly what I expect a receipt to have. It has a description, an amount, a category, a date and the image of the actual receipt. So, when I ask the agent for this, this is how I ask for it. I'm literally saying, "Here's what the user uploaded and give it back to me as a list of receipt data." That's all I have to do. The agent will reason out that I have to parse this image, read the text from the image, figure out what the amount was, what the category was, what the description was, map it into this class and pass it back. That's all done for you. You just iterate off the class that comes back and you're done. Hello, there we go. Okay, so going to talk about Model Context Protocol. Is anyone using that right now in their applications or experimenting, exploring? So, I mentioned it a little bit earlier but Model Context Protocol is a way of describing services, and agents use it to run what's called Tools. Now, if you've worked with large-language models for some time, Functions was a thing that came out and it was like you could pass a local method or an API and have it call that to enhance its response. This takes that into a distributed system now and makes it possible to plug these different pieces together. So, let me show you the example. So, when you book your trip, we're simply telling the agent, "They want to plan a trip," and then giving it a set of tools. And this is the way we pass tools here. So, if you have local methods, for example, there's a factory that's in the extensions library that allows you to say, "Map this method as a tool," and then the agent can call the method and you just have to describe what it does. Here where do we get those tools from? If you look here, we're getting it from the MCP client, so we have both an MCP server and client in this application and literally the prompt is saying, "Hey, you're a travel planning assistant. Consider all these factors," give the itinerary, and then we're giving it a set of tools. I don't even know what those tools are. They could change over time. We might have one airline provider, five airline providers, a multi-airline provider, but the point is that can be registered in your MCP server, passed to the agent. It will make the decision of the right one to call and synthesize the response. It's very, very powerful technology that's built right into this demo. So, having said that, though, this isn't all MCP can do, and that's why I've invited Brady to share with us some creative things he's been doing with MCP.

Brady Gaster: Hey, everybody. How's it going? (applause) So, Jeremy talked about the Model Context Protocol. What I want to talk about is the MCP SDK for.NET. So, you'll see on my screen that I'm actually on the Model Context Protocol GitHub site, and what we didn't

do is go off and create our own repo. You don't have to go somewhere else and find it. We went as far as to just go ahead and put our C# for MCP directly in their repo. They're welcoming to that. You'll see the other ones for other languages out there as well, okay? Now, the way Jeremy was referring to MCP and what you can do with it, it's almost like a -- I like that metaphor a lot. It's like OpenAPI for the LLM. It's a way for you to talk to the LLM in a way that you can say, "Go do something that I want you to do in some other app somewhere." And then we can use it -- use the LLM to tell the LLM something in your native language and have it do something in a system. So, I'm -- well, I like to claim that I'm a musician, okay, and I like to make music, or noise, depending on your perspective. So, what I did was I started learning how to use RMCP SDK for C# by creating a mini server, because why not, right? Let's let the AI help me make music because I'm definitely not creative enough all the time. So, you'll see here on the screen I'm going to dive into this a little bit deeper in the next couple of slides and with a quick demo, also hosted in Aspire like John showed, but you'll see here on the screen that I've got my MCP mini server. Now, what I'm going to do is I'm going to give it a prompt, sort of like, "Play the song "Mary Had A Little Lamb" on the Windows Wavetable device, on the Windows Wavetable mini device," the default mini device we ship with Windows. Thank you, Pete and his team. "Play only the first 16 bars of the song at 128bpm." Now, let me break this down for you in terms of how the LLM knows what to do. So, the first thing you'll see is that I've got this MCP server tool type attribute above my class. That basically identifies that this class is a tool that can be pumped into an LLM and used by that LLM to do things, in my case play music or noise. You'll see down below that I've got the MCP server tool over each one of these individual methods. "Get mini devices, get the sequence schema." Think about that as the song, schema, etc, etc. My songs or my sequences are represented in JSON format obviously because why not? I don't want to use YAML. And then you also see obviously start and stop. I want to start a loop, I want to stop a loop after it becomes irritating. Mary Had A Little Lamb at 128bpms gets irritating in about nine bars. So, the way this works, let me break down my prompt in terms of what the LLM would do. First thing is the song. Okay, how do you tell LLM what a song is? Well, it just pulls down the schema. Whenever I say, "Play the song," the first thing that's going to happen is the LLM is going to call that tool and it's going to pull down that schema and it's going to go, "Okay, now I know what a JSON version of a song looks like." How does it know to do that? That description attribute. That description attribute is almost like a system prompt to the LLM to say, "If somebody says song, this is what a song is. You just call this tool, call this method, I'll give you back a JSON body. Oof, you're off and running." Next piece, play. You see that description? Play is a sequence based on a JSON format. A JSON format, you can get that from, "Get sequence schema," right? See how that works? Now, when I call start, it's going to pass that sequence schema over to it. I don't even have to have a file, it can all work in memory, and it'll actually start to loop whatever that track is. Now, on my device, I've got a couple of different -- on my computer, rather, I've got a series of mini devices. One of them is obviously the Windows Wavetable device, one of them is this fantastic drum machine written by a great company cal

shoot. This thing right here. This is a drum machine. That's made by a great company called VCV Rack. The idea behind that is you could go out and spend tens of thousands of dollars on Eurorack modules and fill up a whole room, piss off your family, or you could use a piece of software and do it all for free and try it all out yourself. So, in my case, I would have Windows Wavetable device on mini device one, I'd have the VCV Rack drum machine on mini device number two. You should all be getting worried right now, I'll put that out. Now, last piece, how does it know which mini device to try out? I'm going to give you back a list of those mini devices using this tool, so whenever I say, "Whenever I play the song Blah, " okay, now I know what a song is, now I know how to play that song once I've generated it, and I know how to pick the mini device to be able to play it on. And, finally, we also have stop, you know what I mean? So, I can actually stop it if I want to. Which, why would you want to stop a great song, you know? So, let me go over here too. Now, going over to my code, again, I'm going to run my code from within VS. We just released preview support for MCP tools running inside of VS. To me, I tried to get that working. I think I've got something odd. I've got an internal build so I couldn't really get it dialed in. I'm going to use VS code as my MCP client but you can do all of this is VS. I would just have two instances of VS open, one would call into the other. I did vibe code all of this. I think I might have written four or five lines of code or changed a couple of method names. I used Copilot in VS to write all of this code for me, so that's not fake. You can really do that. So, what I'll do now is I'll just go and hit, "Run," and this is actually going to spin up that back end. It's literally a minimal API project. I actually have a Swagger front door on it, so I could try the individual method it calls out, since they've got a back-end service that I could call that does all the mini stuff, and I can then call that service from either Swagger or from MCP clients. You can see right here, if I open up that Swagger, here's the mini devices, here's play, here's stop. Just to show you -- I don't actually have the schema call there. So, what I'll do now is I will go right here, so you can see that console log for my mini server and then I'm going -- anybody getting nervous like me?

Jeremy Likness: Yes, getting there.

Brady Gaster: I'm going to flip over here to VS code and you'll see that I've got my settings JSON file, and in my settings JSON file, I've got a list of MCP servers that I'm going to use. My list's pretty short, it's just one, and it's going to talk over to that mini server running directly on my machine. So, I'll just click "Start" right here. Now it's running. I can prove that it's running because you can see that's that MCP client reaching out and talking to my MCP server. You can see it all lining up here. And now I'm going to try something that I don't try very often. Going to try to talk to the computer. Actually, I won't do that. I just want to type it out. So, forgive me, I'm going to type it out. We'll do -- zoom it right here, so I'm going to type this out. One thing I would show you, let me zoom back out again, this little window right here, so icon right here "Configure Tools", I click on that, so I can -- you'll see up here, there's my MCP server. Here's all my tools right here. Where's actual tools? There it is. Where did it go? Looks like it's running, doesn't show my tools yet. Let me try that again.

Restart. Doesn't show me my tools. Hopefully that works. Alright, let's try that. "Play me" -- what shall we play? Give me a song.

Jeremy Likness: The Final Countdown.

Brady Gaster: The Final Countdown. It's the last day. "Play me the first 16 bars of The Final Countdown on the Windows Wavetable mini device at 128bpm." Now, I love to build the suspense, okay? So, before I do this, there was a day where I was trying this out with Glen Condren from the runtime side and I had this set up as a -- I was running it behind a Dev tunnel. So, he could actually talk to it. Now, that got weird, trust me, but when I was trying it with him I would -- I was trying to run the MCP server and I would say, "Start mini playback," and Glen went, "What are you doing? Why are you saying, "Start mini playback?" I was like, "Because the name of the method is start mini playback." He goes, "The description tells the LLM what to call. You don't need to say, "Start mini playback." You don't need to do that. Just say, "Play the song."

Jeremy Likness: Just talk to it, Brady. Talk to it.

Brady Gaster: And he said, "Play the song." So, I said, "Play the song," and it started playing the song. So, I'm going to say, "Play me the first 128 bars of The Final Countdown on the Windows Wavetable mini device at 128 bpm." And also open the JSON file up in my editor. Hopefully it's going to call the tool but I didn't see a list. Oh, look at that. So, now this is saying, "Get me the mini devices. Do I want to run that? Do you perceive this as safe?" Well, it's Brady's code. Maybe. I'll hit, "Continue," here. It's gotten now. Now it's going to say, "Now I need to go get the schema," going to go get the schema for a song. It's gone and gotten the schema. Here comes the scary part. This can take a second. There we go. Created the first 16 bars, it's downloaded the file. It's created that file in a temp folder, which is the file I have open right here. It should open that file up in a second. Bring it down. It didn't bring it down. I'll hit, "Continue." (music) Okay, let's do this. "Add a house beat on the VCV Rack," or, "Use kit drum on channel one." Let's say, "Cancel." How can I cancel that? I'm going to redo that prompt, and, "Don't stop playing." Uh oh. Uh oh.

Jeremy Likness: It stopped playing.

Brady Gaster: It stopped playing, darn it. Oh, well, you get the idea. So, you got the idea, so I'll do, "Stop," just in case it's still playing somewhere in the background. That would be bad. Maybe I didn't dial in my second mini -- but you get the idea, right? You can use MCP to not only dial things in your visual studio, or dial up things in your backing code but you can get creative it too and you can do whatever you want. This shows you how the LLM, like you said -- MCP becomes the OpenAPI for the LLM. You write these tools in C#, you use nice descriptions. That tells the LLM how to call a tool and you talk to it in English and it goes off and does its thing. Thanks a lot for the opportunity, Jeremy. Thanks.

Jeremy Likness: Ladies and gentlemen ( applause), that was DJ Brady with MCP. Okay, here all week, folks. Alright, so how does it get from me asking it to do something to the right place, to use tools, create a receipt, parse a receipt, itinerary, etc? So, first I want to show you the human approval piece. We're literally just mapping endpoints here in the application but we're using Semantic Kernel process framework and I'll show you how that breaks down but here you can see we've got an endpoint, and then that endpoint does some processing and we've got code here for messages, so on and so forth. That's all fun and dandy but what is the heart of this? Well, this is a step, so we have different steps in our workflow and I'll even show you how we get to the steps so I'm working from the bottom up here but, in this step, we're processing the approval and in the code it's really prompting the model saying, " Create a formal trip request based on this. Include all the necessary details for approval. " And then it handles the response that it gets back from the human and says, " Is it approved or is it rejected? " Etc. Now, this all gets wired up right here. When we did that call, if you remember, this was years ago in the presentation, but we asked it for the user intent. So, based on the user intent, we're routing it to the appropriate agent. So, we've got the local guide agent, we've got the policy question agent, we've got the travel planning agent. These are all steps and processes. They don't have to talk to an LLM but most of them do and then it just boils down to pretty much a prompt that goes into that. So, to break this down, I type something, I get the user intent, I pass it to the agent that handles that intent, then I pass the message to that agent with tools and that agent figures out what to do and we go on our merry way. A it books the itinerary, processes the receipts, creates the expense report and you can break that apart in the code. Now, I do want to talk about this for a second. Does anyone recognize this code or write in this language? I'm curious. Do we have Polyglot,.NET, Python developers? So, this is a Python service that Aspire allows us to host. So, you don't have to use.NET Aspire to create distributed Agentic applications but it does provide a lot of benefits. If you were at the session yesterday, you will see that it's being updated to handle more and more environments so it's not just deployed at Azure but it's deployed at wherever your code is running. Now, what I love about this is if you're, again, not familiar with Aspire, there's a very fluent way that you define the resources in your applications. So, here I can clearly see I have storage and here I can see I have an MCP server and I can also see it depends on OpenAI. In fact, we're not even going to start the MCP server until we verify that OpenAI is up and running. The other part of this is that, by defining it this way, it doesn't just get you the ability to run the distributed application but it also can be transformed into resources and I'm going to show you what that looks like. So, I'm going to pop back over here and we're actually going to come out of the code. So, the first thing I want to show you is what it looks like to run Aspire and the way that -- because we've declared these dependencies, when I make a prompt which is this is me getting back from Build saying, " I'm exhausted, book me a trip to someplace in Italy, " just so I can see what the itinerary would look like. I'm not prepared to go to Italy right now, but we'll have it plan an itinerary. It's going to go out and do its thing, but then what I'm going to do is -- in developer mode by default Aspire lights up this dashboard, and you can do it in production mode too but, because of what it exposes, it's a -- there are certain things you need to do it

around it, but here's all of my resources. And when I drill into this chat messages, you can see all of the steps that take place in that transaction. It's called out to the MCP server, it's gotten the list of tools, it's gotten details on a specific tool and then it's gone and passed several times information to and from OpenAI so that it can compose that response. Now, each one of these steps are a step that you can drill into into detail, so you can get the console logs by resource. You can drill into an exception. If an exception occurs, you can identify exactly where that exception was and get the details for that. Fortunately, my code is running completely 100% bug-free so I don't have any exceptions to show you, but if there were, which I -- it's like my friend. They don't always have errors but when I do. Now, this is me getting ready to deploy it to Azure, and I just want to show you, if you're familiar with the AZT AZD tool, Azure deployment tool, this tool is something that I can use inside a.NET Aspire application and literally just run AZD in it. First I ran AZD off login, now I'm running AZD in it. It asks me, " Do you want me to just use the code? " I said, " Yes, sure, " and it says, " Oh, I see you have this project. " I said, " Yes, use that project. " And what this'll do is it'll generate all of the assets necessary for me to host this in Azure. Now, I'm not going to show you this step because it takes a little bit longer but if I were to type, " AZD Up, " this would deploy those resources and give me an up and running set of resources using container apps in Azure. Last but not least we talked about eval. I'm going to show you two use cases for eval. There's multiple use cases but one of them is for safety. That's one of the dimensions it provides from a model's responses. So, here we are in the application and we're getting ready to upload receipts but I'm going to upload not a receipt but an actual frame from a cartoon. And this is a Tom &amp; Jerry cartoon that I can't show you but it shows them engaged in a fight in the street, and you can see that eval has come back and told me exactly why that receipt's not approved. It was mild violence. And we expect our receipts to all be benevolent. We have very friendly receipts, so that just doesn't make sense, and so we blocked it. But if I upload a legitimate receipt, no unsafe content detected and we're good to go. So, that's one way that you can use eval. Another common way is to test things about your model, and so when we're testing using the eval framework, we light it up right inside your test harness, so you can see it as an actual test, but you can test things like how complete is the response? How accurate is the response? How grounded, so to speak. Here I'm testing the retrieval. Because I know I gave it the company policy document, so I've told the test that, " We expect, when someone asks about the policy, for you to give this text back. " Now, eval gives you a set of built-in ways to evaluate different dimensions and here I've got a retrieval evaluator that I'm going to use to say, " When I ask this question and I've parsed in this PDF, does it give me the right response? " And so it will actually measure that and the test will pass or fail based on however I want to set that up, but if I debug this, I get this evaluation report, which is actually going to show me, on a scale from 1-5, how accurate that retrieval was and how complete it was. So, you can see here we got a five, we're doing good, we're going to rock and roll. So, I'm going to leave this screen up for a second for you because this is the link to the demo that I just showed you, with a fancy QR code, and I'm -- I'll wait a couple of seconds and then this deck will be available and we share out the link and everything else. And then the other thing I want to

share, let's see, you got someone taking a photo, someone -- great. Is that, all of these things we're talking about are still ongoing and in November 11th we're going to release.NET 10, version 10. Wow, it's excited. November 11th, mark your calendars. ( applause) And then, finally, this is the link of a walk-through to get started with those templates if you're new to AI or,if you're using AI but you want to see our guidance encapsulated in template format, that's where you can go to get it. We have a very short time but I do think we could probably take one or two questions so I'm going to pop over to that and then we'll conclude. For those of you that need to go to another session, thank you again for attending this one so early and we appreciate you and look forward to hearing from you how we can improve that experience of working with AI. ( applause)

Brady Gaster: Thanks a lot. :

[ Applause ]

END