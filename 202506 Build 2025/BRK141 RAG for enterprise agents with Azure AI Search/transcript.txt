Microsoft Build May 19-22, 2025 Session: BRK141 Speakers: Pablo Castro

Pablo Castro: All right. Hello, everyone. Good afternoon. Hello.

[ Applause ]

Wow. That's a lot of energy for the last session of the day, so I really appreciate it. I'm Pablo. I work in the Azure AI Search team. And in this session, we'll talk about knowledge retrieval and how we use Azure Search to power knowledge in the context of RAG and agents. Satya used this slide this morning to frame a little bit to what our foundry stack looks like. There are a number of moving parts involved in how you create real world agents that solve real world problems and I wanted to start with this slide to orient yourself on you are here in terms of where we are in the overall picture. So Azure Search plays a role in ensuring that whenever we build agents and other AI-based solutions, we do them in a way that are grounded in real world data. So if you think of the Azure AI Foundry as this agent factory, there are many things to think about, in terms of the component parts that you consider when building a solution. There is the models themselves and the role they play to reasoning and understanding content. There is the actual orchestration system that makes agents agents. And among other things, what you need is a system that allows the agent not only to perform the task, but to connect with the real world, where we'll find content about what's going on, what backs the application-based knowledge, or the knowledge in your organization or generally speaking, what is the touchpoint between the agent and knowledge out there in the real world. So what I thought I would do in this session is talk about that element of knowledge and grounding and how do we think about the evolution of it and what are we doing in our products to make sure that we work together with you as we evolve the problem space. About two years ago or so, we started to talk about RAG, or Retrieval Augmented Generation. I actually remember, I think it was two Builds from today where the first waves of this discussion started and we were trying to figure out what the pattern looked like, how do you make it work, and the reality is that if you think about how RAG was constructed at that point, the way I think about it is, it was built out of the tools available in the room. So there was this clever observation of in-context learning, you take one of these language models, create some instructions, concatenate some content that you retrieved and may be related to the question at hand, called the model, the model would answer, grounded in that information. So it was a clever observation and we managed to put these things together by saying, we'll use the language model to actually understand the content and respond and we will use whatever we had at hand, which at that time was a search stack, in order to find the right bits of information to feed to the model. So the result of that was pretty effective, also language models that did work for

search, we used either keyword search, technologies that we had at hand, there was also an acceleration of adoption of vector search, so vector search became popular for good reasons. Vector retrieval is a lot more robust when it comes to vocabulary gaps and things like that. And we can't complain. That got us two years of progress as an industry. We built the first few waves of applications that were successful and solved real customer problems. But as things get more sophisticated, like the feeling that we have, whatever got us here won't necessarily be the same technology that we'll use moving forward. In the end, now a couple of years in, we understand the problem space much better and we think we can do much better in terms of the tools we put in front of each of you, as developers, in order to solve these problems. So in the context of this talk, I can break the problem down of evolving search in three fronts. I'll talk about the query pipeline and how agentic retrieval can take search to the next level. I'll talk about the data pipeline and how we will bring data to the search engines in the first place, so you can answer questions. And finally, I'll talk on the critical element of access control when it comes to agent-based scenarios. So with that, the overall theme of the talk is that I want to emphasize the fact that we started with whatever elements we had in the room and we're transitioning to a purpose-built system, where we go from whatever we adopted to do the job to things that we purposely built and constructed in ways that should make the task easier or better and hopefully it'll get you to a solution quicker. First thing I want to talk about is the transition from search to agentic retrieval. If you think about search the way we use it today, a typical scenario is you use some search engine or search library for the retrieval stage of RAG, and the result of that is you issue a search query, and you get back top 10 results or top-K results for whatever number you want to retrieve. But that's a single-shot event, a linear set of results. So whether or not that fits your scenario, well, that depends on the questions that are being asked. In some applications or for some questions, that would be really good and for others, not so much. Let me give you a complete example. So sometimes in the context of a conversation, someone types a straight-up question that is like a fact-seeking question, single topic, very to the point, like the example that says "Works" where we say, I want to know what the security updates for the knowledge-based article number, number. That we can solve in single-shot search requests and for that we don't need to change anything of the things we're doing today. Now, that's not how many of the conversations we see look like. People are expecting more about what they can say to chatbots or agents when they talk to each other. They accumulate complex questions. And we see things like the doesn't-work scenario, where there are all these moving parts in this question, from talking about multiple things at the same time, to making odd typos, to making references across different parts of a sentence, and whatnot. These are examples where single-shot, just top-K, find a few candidates that maybe have some semantic similarity or something, it's just not going to be enough and we want to address that particular challenge. So let me contrast this. In the classic search approach, usually you take a user query, the first thing you do is you keep what we call the recall stage, search people sometimes say the L1 stage, where what you do is you use vector search, keyword search, pull a few candidates, and that could be what you fill into your retrieval or your language model stage. So this works okay

for some scenarios. It usually leaves a lot of quality on the table. So what other engines do, usually

[inaudible]

strength engines also have an L2 stage, or a reranking stage, where what you do is you use recall to go from millions, or maybe hundreds of millions, of candidates to maybe like 100 or 50 or so and then you use a high resolution, high capacity reranking system to really tune the top results and then you feed to the model. And then finally you feed that to an LLM and again, single-shot. So in a flow like that, you can answer questions where the initial question was formulated such that your first hit into the search index guard the documents that have the answer. If that's your scenario, then we can do this today. I would encourage you to still look at things like reranking and whatnot. They make a huge difference in terms of quality. But you can do this with Azure Search today without having to stitch any of these pieces together. We'll do all of this process for you and the results will be of high quality. Now, there are cases, like the example before, where all these moving parts won't actually answer the question, because the question is simply not answerable in a single search event. And for that, what we are announcing today is what we call agentic retrieval. Agentic retrieval is a shift in how we think about the retrieval task for grounding agents and LLMs. When we retrieve in this kind of agentic approach, we are applying the same methods we use to create agents out there in our own search engine. We do query planning to really understand, what does the information need that is being formulated to us? Maybe decompose the query as needed, find out many questions or different slices of the question so that we can find different data points that may be necessary to stitch together. And finally, we merge all the results in a single response that you can feed to a language model. So the picture is a little different. This time, when you call the system, what you give us is not just a query, you can give us a slice of the chat history, so we have the context. We do the planning, and to do the planning, we use a language model internally as well, where we'll do the composition, we'll correct typos, we'll do a little bit of planning, we'll paraphrase queries so we make sure we have best recall. Then we do everything in parallel, including running all the steps we talked about before. We'll do a recall stage, we'll run the reranking system and whatnot, but we'll do it multiple times as needed, and then we'll combine all the results into a single answer. So the goal of doing this is to, coming back to a question like this, to be able to answer this question in what it feels to you like a single-shot. So in this case, for example, when you have a sentence context, things like typos are much easier to fix. Also, if you apply chat history, you can really contextualize the input. For example, in this case, if somebody says, what does such and such number fix? Maybe before in the chat history there was discussion about this being a security issue, but it's not reflected here. So you are knowing which knowledge base to look for, except that it was just said a few turns before. And then similarly, in some cases, and this one is one of them, where actually the implied information here is branched into two things you have to go look for, but in order to split it, first you need to understand the input so that you know that you

have to seek multiple pieces of information. And second, you have to reformulate these questions because, for example, in this case there are these co-references you need to resolve in order to make multiple stand-alone questions that you can go retrieve for. So it's personalities like this that we think agentic retrieval can make a significant difference, compared to using traditional search systems. Traditional search systems were fine when you had simple questions, but if you want to be able to cover this whole spectrum of questions, we think the agentic retrieval approach is the most general approach to these. And it opens the doors to get more and more sophisticated over time because the contract is really up-leveled. Like where in the past you would just say, here's the one index and I want to go search it, in the context of agentic retrieval, what we do is we introduce a first class construct that we call a knowledge agent. A knowledge agent allows you to describe, where is the data that you want the agent to operate on, which model you want us to use for the reasoning steps, as well as a bunch of policy that controls how the agent behaves. Now, a key difference between this and the search API is what is the extraction level this API operates? This API doesn't say, give me which fields to look for and maybe require to encode a vector so that you can compare with the other vectors and things like that. It just says, tell me what the chat history up to this point is and we'll figure out what is the information needed for the next step. So it's really an up-leveled API. So once you created an agent to actually retrieve from the system, instead of calling the search API, which you can still call for the rest of the scenarios, when you call the retrieve end point in an agent, what you do is you give us the last few turns of the conversation that you're having, including if you have tool calls, then the actual tool call. And then optionally you can give us some parameters around this. And then the answer is, first, our response that you can fill into the language model loop that your agent is writing. We'll also give you activity so you know how we got to this part of the answer. And finally, we'll of course give you references so you can give the end user citations of what are you grounding the answer a particular question on? So this is kind of abstract, so let me show you this in action. So what I'll do is, let's actually use this end to end just in a quick notebook so we can see the different pieces in action. So what I did is I created a search client. I'm just using a Python STK. I have this service called build dot search dot windows dot net. And first, look at what the classic search API looks like. So if I want to search for something like in this case, I have this data set that is for like an outdoors company, so I want to look for -- not for tens. I want to look for tents. And so when I call this search API, what I do is, I issue my search request and I have to say things like what the type of query, how many results I want. If I want vector search, I have to also either provide a vector or provide text to be vectorized and whatnot. What's great about this type of API is that you're in full control of the system. So if you need a custom vector database that is super scalable, or if you need a full search engine that does reranking and whatnot, this is the API to use. And when you hit this API, what we'll do is we'll search for other candidates and give you top-K responses. So super straight-forward and also fast. This thing runs sub-second very often two-digit milliseconds and it's a straight-forward way to retrieve from the system. Now, sometimes you do have these complicated questions to handle and that's where knowledge agents come in. So let's

actually create our first knowledge agent. Typically you do this as part of the deployment of your application. In this case, I'm going to call it the product agent. I'm going to say the data that I want to use comes from this particular index called "contosoproducts" and I have some policy, in this case, where it's a threshold of predictive ranking between the document and the query. And then finally I'm going to say, this is my model for the LLM steps for the query planning stage. So now that I have a knowledge agent, again, the way I query the agent is very different. So I don't really give all these details. I just say what's going on, in terms of conversation and what information I need. And so imagine this conversation where the user says, what are examples of popular tents and the answer, maybe from the system, maybe from a previous part of the conversation is TrailMaster and SkyView are two popular choices, and then the user goes, well, which one fits more people? Typical example where unless you have documents that describe accommodation of the sizes and all the other characteristics across all of your products, there is no one chunk of text that has this answer. If you have a few documents, maybe you get lucky and you search enough that you catch them. On a large dataset, you're just not going to catch all of these parts, because in the end, this job needs to be decomposed. So instead, what I do is I take this history and I feed it to retrieve API. The retrieve API takes the search history and optionally some other parameters to tune what we do, and this goes through this whole process I mentioned before of planning the query and executing it as possibly multiple parts. So let me show you some parts of what the answer to this looks like. So first, we'll give you the actual content that you can feed to a model, or the content is a bunch of reference that can be used for grounding. So for that, we'll give you the actual content. In this case, these are chunks of the original content. Remember, in this case, it was these product descriptions. And also we give you a reference like this so that you can create citations. You can say where you got this information from and then at the bottom of this document you'll see the actual reference pointers. Now in addition to this, we also give you -- sorry for this ugly thing, but I just wanted to show the inside of this response. We'll give you activity of how we got here. Let me show you this. So again, this is what we just saw, but also I can look here and look at -- so what actually happened under the covers when I sent this chat history? So first step was to do query planning. So we understood the query, any paraphrasing necessary, branching to multiple queries, and whatnot. In this case, the system chose to branch the query and you can see that one of the searchers went into TrailMaster 10 maximum capacity and we can see how long we spent on that query and whatnot. And then the other query was SkyView tent, maximum capacity. So this isn't realized that this is a bifurcated query, issued two separate queries, got the top documents of each and then constructed an answer with that, and that last step was a pre-rank and merge all of these together. So by just calling the retrieve API instead of the search API, I've got a system that really understood what the conversation was about, was able to factor that into the query choices, then create a plan that was, in this case, this multi-leg retrieval strategy, combine all the results, and give the results back to the system, along with an explanation for why did we do it or how did we do it? So we think this approach will significantly simplify all these hard scenarios where you put the system together with traditional RAG and it just

doesn't do what you need it to do because the retrieval part was too hard. Now of course the question is, is it actually better? That sounded like a lot of moving parts. Is this a good idea and if so, do the numbers support that? And so, the matter of evaluating the systems is a topic that is actively evolving and is a complicated thing to tell. Gone are the days where we just have this number and it's like, look, we hit this metric and you are good. How do you tell if a conversation is going well? That's a much harder thing to assess. But there are emerging frameworks and one that we like a lot is this idea of these three dimensions that you look into when evaluating one of these systems. They involve a content relevance, so is there content that we are grounded on relevant to the question or conversation we're having? Second is answer relevance. That is, content might have relevance, but is the answer produced by the language model actually relevant to the question? And then finally, groundedness, which is like, is the answer that I got based on the data that we found and not hallucinated or coming from some other place? And the goal is to do as well as we can in all these dimensions so that we know that when you turn this on you're getting better results than you would otherwise. So this gives us a framework to evaluate the work. So agentic retrieval really performed well when it comes to complex questions. So when we measure the number of datasets that we use to track this work, we see a 40% increase in answer relevance and also 30% increase in result rate. So what that means is, when the queries get difficult, there is room to do better. An agentic retrieval approach is delivering on that particular approach. Of course you have to test this in your own scenarios, but the early signs of all the valuable evaluations where we do our best to make the datasets we evaluate on look like real world scenarios, we see very strong results. There is a link at the bottom of this slide where we have a blog post with a lot of detail on the methodologyOnscreen text (aka.ms/aisearch-arevals) and the details of how we evaluated this and a lot of data. Let me just fly through a couple of the details. So first, we do this across a whole bunch of datasets. We use things like a support dataset. We also have this MIML dataset, which stands for Multi Industry and Multi Language. So we tracked this matrix of multiple industries like finance and manufacturing and whatnot, and also multiple languages and we ensure that we are doing well across the entire matrix. So what you can see in this slide is, the top half is classical simple queries that you can do well with a single search. The second half is complex queries. Let's define complex queries as queries where you cannot answer the question by retrieving any one single document. So you can see that in the complex questions, we have material difference across all the datasets that we evaluated. So we are seeing very strong results, in terms of immediate boost in quality when you have complex questions to deal with. On another dimension, we do this across a whole bunch of query types, because we want to understand in what cases this technology applies, when it doesn't, and when it could actually regress the whole experience. So good news. Very rarely we make things worse. And of course, things like simple keyword searches, we're not going to do better, because there's no farther semantics to exploit when you have just a couple of words, but we want to not make it worse. And then across the board, when you have things like multi-hop questions, then we do significantly better. And finally, this slide is boring because it's all gray and that's how we want it. What we want

is to not regress the groundedness position. If we're doing well in grounding, we don't want these complex queries, we have complex agentic retrieval system behind them to do work and reduce risk of hallucination and whatnot. And what we see is that we stand at the same level in groundedness that we were on simple queries. So overall we're very excited about the start of agentic retrieval. We have a lot more work to do there and one of the things I really like is the agentic retrieval abstraction, this idea of a knowledge agent that has a much higher level program and interface gives us room to also innovate more there, because now we can do a lot more in the internals without turning the API or anything, and deliver more and more capability in this space. So we talked about retrieval, but the reality is that you can only do as well in retrieval as the quality of the data that comes in in the first place. So while there's all sorts of things we can do on the query side, it matters a lot how the data is brought into the system. So there is a number of things we are doing in the context of data acquisition now that we understand the RAG pattern well and we can do better on RAG-specific data ingestion capabilities. So again, going back to two years ago versus today, when we started the RAG wave, the pattern was, take some text, chunk it in 500 token chunks or something like that, then put that into vectors, and it'll be all good. Then put that in an index and retrieve. The reality is that's not what data looks like. Real world data, like business documents, they have diagrams, there is information in the layout itself. Sometimes you have complex schematics with arrows, and text, and boxes, and whatnot, and you have to figure out what is the content behind all of that in order to do a good job retrieving. So we want to help with these scenarios so you don't have to manage them yourself. In the particular context of these complex multi-modal documents, we are covering a whole bunch of new ground and things that you would have had to do yourself before, so we're excited about putting all of this together in this kind of multi-modal wave. So first is, we did a lot of work on image retrieval. We did a lot of work exploring what's the best way when you have a document that has text and images to also find the best images when the information to answer a question, maybe it's in the image and not in the text. One pattern for this is using image embeddings or multi-model embeddings. They are these vector models that makes images and text. And that works well in some cases, but there are many nuanced cases where that is just not the right strategy, especially when you have many diagrams that look the same, but there is a lot of nuance in the details, like where arrows point at, what is the text behind the arrow, things like that. And what we found is that actually verbalizing the content can work really well, verbalizing as you look at the image, and you say what you saw. And of course, you can't put people to do this, but we can put LLMs to do this. So we want to make it easy to have that in your ingestion pipeline. The other thing you typically do is, if you have text and images in your document, when then you're using this information, say in a chat application, if you grounded an image, you need to show the image, so you can explain where you got the information from. But the image is embedded in a document. So somebody has to run all this code to decompose these documents and all the individual parts and make the individual components addressable. And even then, it's not just images. You want the whole layout information to be captured so you can exploit it during retrieval. You can use it to automate reasoning. You can use it to

understand the nature of the content. And finally, chunking is not going anywhere. You do have to split these things into parts and we need to constantly increase the level of sophistication. You need to do that. So the question is, who wants to do all of this? You probably just want to be done with your application. So we've been doing a lot of work not only to add the component parts, but to make it so that the end-to-end scenario where we understand it well is something you can accomplish quickly. If then you want to customize a bunch of stuff or use the building blocks in another way, go for it. We make APIs so that the component parts are super flexible, but we want the end-to-end to be easy, especially when you're getting started. So let me show you this in action. I'll use the same Azure Search service I used a minute ago. I'm here in the Azure portal. We were using this Contoso Products index a minute ago, but let's create, in this case, a new index. So what I'll do is I'll go to Import and Vectorize Data. In this case, what I have is, I have a storage account that has a whole bunch of PDFs. I just actually took Azure Search documentation and rendered these as PDFs. And it has diagrams and things like that. So what I'll do is I'll say these particular data is in blob storage and now, not only we'll have like a RAG scenario, but we have a multi-model RAG scenario kind of flow that is specifically aimed at this scenario. So this is my subscription. I'm going to say this is my storage account for this case. The Azure Search docs container is the container we were just looking at. And the rest looks good. So that's where my data is, the first step. So what we do now is we take a look at the container and see what kind of data is there so we know what to do next and what options to propose and whatnot. And while I'm doing this against blob storage, I could be doing this against OneLake, or ADNS, or Cosmos DB, or SQL, you pick. Next step is, I want to say, what do we do about extracting layout and other details from the content that I'm putting in. I can do the default, which is a simpler system, or I can use the full power of AI Document Intelligence to extract all this extra information from the documents. So I'm going to go with that now and when I do these things to say which is my instance of AI services that has document intelligence. I'm going to say it's that one. I'm going to authenticate my system identity. And then as I mentioned before, sometimes you want to do vector embedding type approach for retrieving images, but sometimes verbalization is actually better. So in this case, what I'll do is I'll use image verbalization. What that means is during the ingestion process, whenever we find images, we'll take the image and we'll send it to an OpenAI GPT model, and we'll ask the model to describe what it's seeing, and then we take the text back and then we index the text. And it turns out that sometimes that's better than the image. Sometimes it's not, and sometimes what's better is to do both. So we are in the business of giving you options, depending on your scenario, what's going to work better. It also turns out that this is a super useful general purpose building block, which is during ingestion. If you want to call a model, give it a prompt and make it do something, that sometimes saves you a whole bunch of work. You could say, take this input and summarize it, or maybe find some latent structure in it or in this case, do something with a picture. So I'm going to do the image verbalization approach. And for this, I want to point at my OpenAI instance, because I want to give it a model. I'm going to give it a GPT 4.0 model to do this work. I'm going to say that it is system identity. So now we have this step in

the pipeline that will extract a description, a verbalized description of pictures and then we want to actually search the text that results from this. And so what I'll do is I'll point again to this, to my Azure OpenAI instance and say, use this embedding model for that step. Did I forget something? Oh, yeah. I have to say yes to these. So finally, as I mentioned before, there are cases where if you're writing a real application, you have to be able to address the actual content, the bits and pieces of the document, so you can render appropriate application experience. So if you want, you can enable this option and we will do this for you. This is called Knowledge Store. And the idea of the Knowledge Store is that all of the knowledge we accrue during indexing that we land on the search index, we can also put the parts in a storage account. In this case, I'm going to use this storage account called Azure Docs Resources. Authenticate and then I hit next. Then finally, for ranking, I'm going to say I want Semantic Ranker. Generally, you want Semantic Ranker, if you're not sure. This is the ranking stage that improves quality of results. All right, so that was a lot of clicking, but at the end of this, I'm going to call this index Azure Search Docs. At the end of this, what we have is a complete ingestion solution for multimedia scenarios. So what these systems will do is it'll create a pipeline that will take all the documents from the source, parse support file formats, extract pictures and layout and all of that, index all the content in Azure Search, and also it stays running. You can say how often you want us to check for changes and we will incrementally maintain that index over time. So this is not a one-shot demo thing, but it's an industrial- strength solution for data ingestion and RAG scenarios. So if I look here, this is the index that I just created that is running and in a second it'll finish. And once it finishes, if I go to indexes, I can see that now I have this Azure Search Docs index that is brand new and even though it's still running, I can start to see some data come up. And you can see that, you can eyeball this. There's some titles, some content and then a long vector there, which is the embedding for the content. So this is ready to then put a RAG-based application on top that will take advantage of this. Now, because it's also a lot of work to put these applications together on top, you can do a simple demo quickly, but building a full application needs all these other considerations around experience, and grounding, and whatnot. And we wanted to make it easy for everyone to start. So for this multimedia scenario, we also wrote a full sample applications and we put it GitHub Repo; we'll include links to all of that at the end. But the goal of this is to make it so it just works with the output of this wizard I just used. So let's actually go try that. So what I did is I just cloned this repo locally. Hopefully I got the index name right. It needs to be the same as this one. So this is my configuration. What I will do now is we'll just run the app. That's not how you write Python. So the app will just take a second to start. And in this case, I just didn't modify it, so it's just like a general purpose app, but the idea is that you can customize this to your scenario. So let me look here. So I didn't change this. You would change this to whatever is the questions in your space. So remember that what we indexed was a bunch of Azure Search documentations. I've been curious about how we translate query text into query trees, so let me search for that. It may take a second to start cold, because I just started the app, but what this will do is remember that I created this index with all the documentation that included text and images. So what this will do is take my query, go find

the right text, or images, or whatever that's grounding information for this, feed it to an LLM, and then let the LLM produce an answer. So once we have the answer, in addition to the answer in text, I can see that this grounding information, that it came in pictures, which were pictures from the documentation. So first, this means that we found not only relevant text, but also relevant pictures, and then we pull them and fed them to the language model. And also that we made them available as stand-alone parts. No document has this alone. This was part of some big PDF. But we extracted all the images so it's easy to display. So this same application, you can just clone and use on your own data by just running the steps I had run before on the portal, and you'll have a starting point of an application right away that deals with all of these media details.

[ Applause ]

Thank you.

[ Applause ]

Thanks. Alright, so. Of course, the other aspect of this is, that was great if the data was in the perfect spot, but what if it's not in the perfect spot? When the data is in Azure, Azure Search has native integration with the rest of it, whether it's in blob storage, OneLake, SQL DB, Cosmos DB, whatnot, we have deep integration with all of that. But of course, there is a lot of data out there, outside of the Azure ecosystem and often the further it is from Azure, the more shaping it needs before you can put it in a system like Azure Search. So to address these, rather than trying to handle this one by one, what we did is we've been working together with the Azure Search team and the Logic Apps team to make it so that it's really easy to use Logic Apps to feed data into Azure Search from the variety of data searches the Logic Apps supports. So what that means is, now you can point Logic Apps to one of our services and use all the capabilities of Logic Apps to actually ingest the data. So let me give you a quick example of this. So I'm back to the Azure Portal in my Azure Search service. I'll do another one of these import wizards, but this time, instead of choosing one of the Azure Searches, I'm going to say, my data is OneDrive for Business. So here, I'm going to say "One D Contoso." We'll keep going at it with the Contoso topic. And I acknowledge that. And then what I'll say is, because we need to index the data. We need that embedding model for this, so I'll choose my OpenAI service, click embeddings and say, managed identity. Click next. So now I set up the Azure Search half of this to say, this is where the data is, is in OneDrive for Business, and I want to use Logic Apps to actually do all the ingestion process and whatnot. So we'll set up that part of the system and then the next step is to go to the Logic Apps side and set up Logic Apps to handle the exact workflow I want for ingestion, and that'll take a second to load, and then we'll go there. There. Maybe more than a second. All right. So let me actually hit here. So we're continuing Logic Apps. So now the last thing I need to do is connect the dots. Which OneDrive for Business and which part of it? So in this case, in Logic Apps I'm already connected to Azure Search. I'm connected to my OpenAI se

I'm going to say, connect here, because I'm going to use my actual Microsoft account for this, then I'm already logged in, so I'm going to say yes to that. And once this connects, the last thing I need to say is, what do you want to index? In this case, you can see actually my local clone of this where my Microsoft account I have this Contoso folder, so I'll just say Contoso here. And I'm done. So at this point, what I have is, again, I have a setup that will continue to run over time and I can manage like any other Logic Apps system. But what it does is it does all the process for tracking changes, chunk vectorize the data and then pushing it into Azure Search. And you can see here, the first steps were run and now for each of them, we're going through this process. And at the end of this, I'm going to have a populated index that has all the output from the Logic Apps process there. So it finished, so actually if I go here, say close. And if I go to "One D Contoso," this was the thing we came up with, then who knows what the vector means. But you can see, I went end-to-end, from zero to getting data from OneDrive in just a minute. So hopefully this will lower the bar for how you connect to other data sources and bring data into Azure Search for your prior applications. So we talked about queries, we talked about data ingestion. The third part of the picture was enterprise-grade security. So an interesting effect of all this super-smart retrieval systems and the copilots we build on top of them is that they'll find everything. So it becomes super important that you have proper access control policies in place across the entire stack. Now, often the data you're starting with has access control policies. If they are files, maybe they are secure with the names of the people or the groups that can access it and whatnot. But when you index that into a search index, you need to propagate that access control information. So the central question boils down to, everybody expects that if you are not allowed to access a piece of document, or a piece of data in general, then you shouldn't be able to access answers from a Copilot or something that are derived from that information. But that assumes that, say you're using some vector database. If the vector database just retrieves from all the vector pool, then how do you know that you can actually show that information? You could post filter it, but that means you have to do the work afterwards, or you could write filters in the database, but now you own controlling all these details. So what you want to do is, you want to make it so that you can propagate the access control information from the source, land it on the index, and then every query can make it so that it enforces the access control policy that was there on the source. We have some building blocks to do this. In databases you can also apply filters and whatnot, but this is an enormous amount of work. You have to do all this work to integrate your identity system, you have to deal with group expansions, and all of that. So doing this manually is complex and it's error-prone and because it's security, error-prone equals dangerous. So I'm excited to announce that now we're introducing native support for Entra-based document access control.

[ Applause ]

So what this means is, now what you can tell us is, which users and which groups can access this stuff, and we do everything else. So we will automatically do group expansions,

we'll do the filters and for a given user, the index looks as if the only documents that are present in the index are the ones they are allowed to see. And if you update a group, for example, then once the group membership information propagates, then you will automatically be in or out of seeing a particular document here or there. So we did this through three custom fields. You can give us the list of user IDs, the list of groups that can see the content. And we also have another option where if you express your permissions in terms of RBAC roles, for those that work in RBAC roles in Azure, you can also say, here's say, a storage account that has a given scope, check the same RBAC privilege that you would have used to check that particular resource. So this allows you to kind of have the search content follow whatever is the container used to index. Let me show you this in action. So again, I'm going to get a search index client and what I'll do is, I wanted to compose this step by step, because I want to show you exactly what's going on under the covers and how simple actually it is to get this set up. So let's start from the very beginning. We're going to create a new index. This time I'll use the API for this, so you can see step by step. So I'm going to call it "acl demo" and I have an ID and a text field just so we see what's going on there. And then I created these three fields. It's called users, groups and roles. And these ones are marked specially to tell the system that they play a role in the document level security system. And then at the index level, I'm going to enable the permission filters, so that they are actually applied at one time. So let me run this. Ignore the ugly work-around here. By the time you download the SDK, you're not going to need to do that. So now what I'll do is I'm going to connect to MS Graph, so this is a graph API that allows you to navigate many aspects of the graph, but among other things, your Entra users and whatnot. So I got two things. One is I got the me object and the other one is the groups the me object is member of. So let's take a quick peek. So one is, I'm going to enter it through the groups, I'm going to filter it to some safe things. I look for news, so these are safe groups. So there's a couple of groups there that I'm a member of, just effectively all the groups in Entra you're a member of. You can also look at the actual me object, so if you go, me, the given name, this is actually the identity that represents me because I'm logged in. And there is a whole bunch of properties here, where you are, what's your job title, whatnot. You see a fax number. I wonder at what point no one will know what that meant. So now that I have an index and I have the identity information, let's actually create a few documents in Azure Search. So I'm going to create three of them. And this is, by the way, all it takes to upload new documents to Azure Search. So I created the first document, I'm going to say, my identity can see it. A second one, where I just chose some generic. That's not me, so I shouldn't be able to see that. And then a third one where I didn't include myself, but I included the idea of one of the groups I'm a member of. So let's actually go index that. So now what we can do is, we can go search and in Azure Search, if you search for nothing, that means you search everything. So it's like down pull the results. So the key difference now is when you search, I'm using this kind of token provider thing that says, this is the token of my identity as a user. So what I'm telling Azure Search is, I'm authenticating with application identity, but I want the search to run on behalf of this particular user. And so when I do that, if I run that particular search, what will happen is, I'm authenticating as the

app, but my user tokens would use to run against the user IDs, and group IDs, and whatnot, and that's the identity that gets to see or not see any documents, which may be coming from the application or coming from, maybe you propagated the identities from somewhere else. And you can actually see, if I don't use the feature, and I go directly to the application identity, I do see all the documents out of there, but as long as I'm providing the user's identity, I only see the ones that are visible to that user. So this is a huge amount of moving parts that you can forget about, you don't need to deal with, because the system will automatically do it. And of course, I show you the example here, supplied with explicit -- we call this a push API, where you tell us who can see what. And if that's how you want to manage your data, that's great. Then this is first class support. But there are also cases where you already have the documents with permission information sitting somewhere. So for those cases, we support out-of-the box bringing this information as part of the indexing process, so we propagate the ACLs from the search into Azure Search, without you doing any of this work. So the first thing we're announcing is that we support this for Azure Data Lake Storage Gen2, which has native ACLs support. So let me just show you real quick what this looks like. So this is an Azure Data Lake storage account, and if I go to the storage browser for a second, you'll see that I have this container with the same kind of product files I've been using. And if you go look at one of them, these particular containers, because they are enabled for Azure Data Lake Storage, they have access control lists, so you can see who can see what. This could be users or groups or whatnot. These are Entra identities. So if you take one of the indexers like the ones I showed you before, but you point it to ADLS and you enable access control information, then go with transparently to transport this access control information from where the documents are stored into a search index. So that means that you don't even have to do anything on search. The search index will follow the permissions of the source. And I included here some details for how you configure this. But the important thing to keep in mind is, on the index you create a schema with users and groups, and then if you're going to push content, then you give us the IDs. If you want us to take care of propagating ACL information, then just map the fields in the search that have this information into the target and we will then enforce these at query time. So that covers access control. Now, there is another feature that is powered by Microsoft Purview that can play a critical role when it comes to protecting information inside an organization, because there is this capability, we call them document sensitivity labels, and what labels do is they allow you to effectively categorize documents, and as a result of the categorization, this will say which policy, which security policy you want to apply to these documents. For example, you can distinguish documents that could be public from those that cannot leave your company, or you can say only a subset of the folks in your company can see a particular document, and you can also say what is doable with the document. Can you extract parts of it, or not, and whatnot. And because you need to enforce all of these, first, the interesting thing is, this information travels with the document, so while ACLs are the property of the container of the document, sensitivity labels are inside the document. So that means first, the document is encrypted, so if you don't have first class support for sensitivity labels, you can't index it at all because you have to decrypt it first. And second, once you decrypt it, you

have to enforce the policy that the document intends, or you're breaking the rules of the system. So also today, we are announcing this in private preview, where if your organization uses documents and sensitivity labels, we will support them end-to-end. You can drop them in the right container, we'll propagate the policy for sensitivity labels and we'll deal with the actual mechanics of indexing, including encrypting the documents and so on. This is very early, but we think this is a key step in creating systems that are high stakes, where security is critically important in your organization without you having to take the complexity of building the whole underlying machinery to handle encryption, enforcement, and whatnot. So this is in private preview. If anybody's interested in testing it out, just fill out the form that is pointed at this.Onscreen text (aka.ms/AISearch-senslabels) There is also that link you can follow and we'll follow up later if anybody's interested in trying this out. Thank you.

[ Applause ]

All right, so we talked about the three topics we set out to cover. We talked about the query pipeline, the data pipeline, and the enterprise security. I thought I would close with a little bit of a different thought. The way you consume a lot of these APIs today is either through the Azure Portal, like I showed you during this talk, or using code, like I also showed using one of the SDKs. We also have the same thing, everything I showed you can do, of course, in Azure AI Foundry and all the other sessions here are showing that as well. But there is this interesting shift, where through mechanisms like MCP, like the Model Context Protocol, now you can create not only MCP oriented solutions to end users, but you can create things that are oriented towards developers. And that really changes the options for how you build these applications. The Azure team created the Azure MCP Server. The idea of the Azure MCP server is that it fits really well development workflows where the MCP server knows how to connect to some of the Azure services, including managing core resource groups, but also things like SQL, and Azure Monitor, Cosmos DB, and of course, Azure AI Search. But the thing that made me reflect on the result of these is, how you think about APIs differently if the thing that is going to be coded against your API is an agent, along with a person and not just a person, because there are certain things you can do to make life much better. Two examples of these are APIs that can self-describe do much better. And second, if in addition to structure, you can describe as the author of a component of the system, you can describe the role of the component, then you can help agents downstream do their job easier and faster. In this case, let me show you two things that are interesting that you can do. And I start a new chat. So we can start using the same Azure Search service we were using a second ago and what I do here is, I just have this VS code in agent mode with the MCP server, like the Azure MCP server set up, and I can say, what Azure Search index should I use in my build service for the outdoor gear app? Great. Let me try one more time. I actually show it here. This will take a second the first time. It has to authenticate and whatnot, but the idea is this MCP server gives VS code the ability to run primitives of the value of Azure Services, in this case, Azure Search. And for Azure Search,

there are two things that make this particularly well-integrated. First is, if I go back to a portal while that runs, if you look at one of the Azure Search indexes, let's go to this Contoso one we've been using, the indexes have structure, they have schema information. And schema information is not just the fields, but whether the fields are searchable or facetable, or they can use for filters and whatnot. So there is a lot of information that gets you started if you're building an application, because you know the structure of the data. The other thing in the category of the tiniest features that can give you a boost and quality of life for the development is, we actually made sure that all the top-level resources in Azure Search have these description fields. And while in the past, you would have not ignored this and I always left them empty, if you actually talk about what's going on in that index, what type of data it has, how they expect it to be used and whatnot, then other systems can consume this information and for people, you would have put this somewhere else, but for an agent, having the information go with the service makes a natural mix. So if I look here, my question was, what indexes do I have and it says, these are all the indexes you have, and I could say, which one do I use for the outdoors products? And let's see if this time around this goes well. So it knows and it knows because the inscription said something about what is the nature of the content that it has. So now, these agents can talk to the resource model and learn more from the model and explore that information. And you can keep going. You could say, what does data in that index look like? And so through tools, the models know that, for example, there is a describe end point and actually here, it got extra clever, and it did two things. One is, use the describe endpoint to describe the schema, so the structure. And then it just came up with this and actually I did not expect this, but it also just ran a query to sample the data. And then it can describe what the data actually looks like. So in a few interactions, you accumulate a good sense of what this search service looks like, what it does, what is the structure. And you can actually do a slightly more complicated prompt. Like before, I asked them all of this, so basically I have this next JS default template application. I had told it, I have this application, build me an app with this default application, and what do you know about this Azure Search index? And I let it run for a while and this thing really cooked. So the result of this is this app that -- I'm highly incompetent in anything that has to do with UI, so I would've never been able to do something like this. So every pixel you see here, it was just that prompt. So it built an app. It actually chose to do logos and whatnot. It exploited the facetable fields, so that's why it has facet in navigation. It has a default view. I can search for things and it actually hits the search index. You can see it narrowed the facets. It used a little bit of the metadata to describe what's going on. Even in an index that is actually not super well fit for this kind of experience, it managed to create a whole application with it out of the description that was built into the index itself. So, food for thought, in terms of how we're changing how these research models present information that they are backed by, so that it's more effective for agents to consume this information and help you build applications faster. So with that, my call to action is, all the things I talked about today are backed by a blog post with a whole bunch of detail that the team put together and there is code in GitHub for most of the things that I touched on, so

you can clone the samples, the demos and whatnot and try all the things I talked about today.

Onscreen text:

Agentic retrieval announcement (aka.ms/agentrag)

Agentic retrieval eval results (aka.ms/aisearch-arevals)

Multimodal updates (aka.ms/Build25/aisearch-multim)

What's new in Azure AI Search (aka.ms/AISearch-new)

Private preview sensitivity labels (aka.ms/AISearch-senslabels)

Agentic retrieval demo (aka.ms/ragchat)

Multimodal demo (aka.ms/aisearch-multimodal)

Azure MCP repo (aka.ms/azure-mcp)

Foundry MCP server (aka.ms/foundry-mcp)


All the things I mentioned are in public preview, with the only exception of labels, which means you can get your hands dirty with this right away. We also have a whole bunch of sessions at Build that cover different aspects of foundry and a lot of focus on retrieval as well. I'll do an AMA tomorrow in the evening for anyone that is around. We also have a number of sessions that dig into agentic RAG and how to combine RAG and Azure Search with Foundry and whatnot. And with that, I really appreciate you staying through the last session of the day. It was great to have a chance to talk about all these topics for all of you and I hope you have a great rest of the evening. Thanks.

[ Applause ]

END