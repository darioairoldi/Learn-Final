Microsoft Build May 19-22, 2025 Session: BRK195 Speakers: Mark Russinovich

[ Music ]

Mark Russinovich: All right. Good afternoon, everybody.

[ Applause ]

That was actually pretty good for post-lunch on the second day, so thank you. How many people is this the first Build? Wow-- that's a lot of people, first-timers. Well, welcome to Build. How many of you have seen a previous edition of Azure Innovations or Inside Azure Innovations? Quite -- I guess you've watched it online. It's the only way that math works.

[ Laughter ]

Welcome to this edition. I just want to get upfront, that what I'm going to be showing you is some things that we are about to ship, some things that we're shipping, some things that we might never ship. It's really a look at some of the creativity that we've got going on across the Azure platform, and every one of these, which I've been doing this kind of talk for 10 years now, close to 10 years, is completely new, so you haven't seen anything in here before, specifically. You might have seen some evolutions of the things that you've seen before. In fact, I'm going to show you and tie it back to Build two years ago, where I announced something that I'll show you a progress update on, but it's really fresh content, kind of a sampling across the platform, and that sampling includes... a look at some infrastructure innovation, a look at Cloud native innovation. I'll talk about cloud confidential computing and what we're bringing to market there, the kind of innovations that we've got going on software or hardware there, and then I'll conclude with a look at something completely speculative, that Microsoft Research is working on, that could potentially be some kind of future computing platform that probably none of you have ever heard of before, it's so out there. So let's get started with Azure infrastructure, and I want to start first by going inside of Azure Boost, which you might have heard of. That's our accelerator offload card, and to put what that offload card does in perspective, you got to compare it to a traditional IT infrastructure, which is, you've got the server, you've got the virtual machines on the server. You have all of the software that processes the I/O stack, storage and networking, for that virtual machine, whereas when you talk about offloads, you're moving all of that computing of storage and network and the data path onto accelerated hardware, and you're taking the control plane, the things that set up the connections between the virtual machine and that hardware off the host as well. You get a lot of benefits from doing this because now you're freeing up the server cores to actually do server things

and not do this I/O kind of intensive processing. This is an evolution Azure has been on for over -- well over a decade, and we've got something called Azure Boost 2.0 that we've been rolling out into the fleet. This is a picture of the latest version of it. It's kind of, think of it as really the third generation or fourth generation of Azure Boost, where we started with accelerated networking and then Azure Boost 1, and then this Azure Boost 2. Azure Boost 2 here, this card on the left side, you see two ports. Those are 100-gigabit links there, and then you see that big block in the middle. That's an FPGA where we do our accelerated storage and networking offload, and then on the right side, that's an ARM processor complex there, where we've got some ARM cores and DRAM connected to it for the control plane agents that I talked about. The advances we've made, now that we have this thing in the fleet, first of all, Azure 2 increased bandwidth and IOPS across remote storage, local storage and networking, and we've been upgrading Azure 2.0 as the fleet is running. Because it's FPGA, we can upgrade the data plane and provide even more capability, and so these are the latest numbers, these are cloud-leading, 14 gigabytes per second for remote disks, 800 -- with 800K IOPS, 36 gigabytes per second for a local SSD, so if you're attached to a local SSD with Azure Boost, and 6.6 million local IOPS as the peak for local SSD performance. The other benefits we get out of this Azure 2.0 architecture, this Boost 2.0 architecture, is with that dual links, we're going up to two top-of-rack routers, and that means that we can tolerate failures of a top-of-rack router without completely having that server being disconnected from the network. We can also do maintenance operations on that top-of-rack router and temporarily have one of those links be down for just a short period of time while we're upgrading another one. This has done a lot for resiliency as well as for our maintenance being transparent, and then we can also upgrade the software stack, including the data planes, with sub-second updates as well by having that off the card. We also get a very strong isolation. Again, the components that are running on Azure Boost, on those ARM cores or on the data plane, are not affecting, in any way, what's going on on those server cores next door. Like I said, we've been rolling this out, and Azure Boost is now spread across 20% of the fleet. Every single new server has an Azure Boost card installed with it, and we've also added support for GPU and HPC SKUs as well, so they're in those SKUs as well as their large memory SKUs as well, the M-series. For networking, we've got up to 200 gigabytes per second, those 200-gigabit links, we've got 400K connections per second support, and those connections actually involve the ARM core processing that set up those connections on the data planes, and we have same kinds of improvements in servicing that I just mentioned. One of the things that I'm here to talk about, which is a new innovation we're bringing out soon, is the support for remote direct memory access or RDMA, ability for two virtual machines to talk to each other through RDMA rather than traditional TCP/IP. If you take a look at traditional networking between virtual machines and a public cloud, you've got the application that's going through a thick network stack, TCP/IP, a driver -- network driver, down into the accelerated offload NIC, up into the same kind of stack on the receiving side, and this is a lot of layers of packet encapsulation and retry logics and timing windows that slow down in the cases where you know these two applications are going to talk to each other and they want to share information very quickly.

RDMA is a transport that allows the application to map parts of its memory, directly onto and make it accessible to the network devices, the NIC, so it can just copy directly from memory on one side, and on the other side, dump the data right into a memory buffer on the receiving side, and so now you've cut off all of those layers. This is very common in HPC computing, but not in general-purpose computing, but now we're bringing this, especially over InfiniBand or back-end networks in HPC, but with Azure Boost now, we're adding this to our public cloud in general, and what this does is allow even devices like GPUs that support a product called GPUDirect to directly copy data from GPU memory into GPU memory across the network without any of that intervening stack. In this figure you see here, is the virtual machine talking to the control plane, to set-up the memory buffers, to point into the GPU high-bandwidth memory, and this means that now you can do things like AI training very efficiently, because one of the operations is GPUs exchanging weights in something called an AllReduce operation at the end of a state -- a step, and that step or -- actually causes the synchronization across all the GPUs. Now they can just go directly and share those weights directly with each other. Let's see a demo of guest RDMA between two GPUs compared to passing the data across those two virtual machines in the traditional way. I've got this mpi run test up here on the top that is going to be running the standard kind of connection of an AllReduce operation across two virtual machines, and you're going to see here in this test, that it's going to be using buffers and a number of buffers that it's going to be doubling every step. There's six steps here. You can see there's four GPUs. It's going to be talking to four GPUs on the other side, and we see for each one of these batches of buffers, with the number of times of that buffer that we send across the network, the total time and the bandwidth that's achieved, and this is what you're going to see today in Azure virtual machines, talking to each other and doing an AllReduce operation, this, we'll see a summary here, and you see that we've got 51 microsec or -- or 51,000 microseconds, an average bandwidth of 1.4 gigabytes per second. Here, you can see average bandwidth of 14 gigabytes per second and a fraction, so an 11 times speed-up by getting rid of those software stacks related to networking, allowing those GPUs to talk directly to each other. That, like I said, is coming soon in Azure. Feel free to clap.

[ Applause ]

One of the other things that we see on a server, when you got virtual machines is needing to update the software on the server, and everybody is familiar with, when you need to update the host operating system or the hypervisor, you need to reboot that server, and so these are update technologies that Azure has had and has evolved over time, starting with, when we started Azure, all we could do when we needed to update the host operating system is reboot the server. Of course, microcode updates that come for CPUs, that's always been impact-less, so microcode updates don't have any impact on virtual machine uptime, which is our goal, is to keep those virtual machines up 100% of the time. Live migration, we added somewhere 5-10 years ago. We started building live migration into the Azure fleet and now we use this very regularly in the Azure fleet. That has, for the most part, very little

blackout windows as virtual machines do that transition from one server to another, but we need tools that allow us to have the minimum impact and target specific types of updates, for a specific type of update. Over time, we've developed hot patching technology, which is now part of Windows in Azure as well, where we can hot patch pieces of the hypervisor, also pieces of drivers in the kernel, and when the hot patch happens, it's completely transparent to the virtual machines running, so this allows us to do security fixes and performance fixes very transparently and very easily. We've also, and I talked about this at a previous innovation session, is introduced something called a hypervisor hot restart. Here is where we take Hyper-V, we create -- take the new version of it, load it up, and then do a state transfer from the old version of Hyper-V to the new version of Hyper-V, so they're both active in memory, transfer state, old version gets set down, new version takes over the virtual machines. No impact on those virtual machines. And just recently, we started using driver hot patching or, sorry, driver hot swap. Same idea now, but for device drivers, where we load the new version of a device driver into the kernel. It's side by side with the old one, and then we have the proxy stubs in front that take over and start routing requests to the new one. The old one's requests get drained, and then we can unload the old driver, and that also let's us update drivers with no impact, and this makes it much easier for us to keep the fleet fresh. What I'm going to talk about here is advances to something called virtual machine-preserving host update, or VM-PHU. That's a technology I introduced in one of these sessions, probably back in 2014 or something, that we've been evolving over time, and we've got multiple versions of VM-PHU. VM-PHU, what it does is leave the virtual machines in memory, all of their state, reboots the system around it, and then resumes those virtual machines, so we call this fast save and fast restore in Hyper-V. One of the things that we've done is made it so that we don't even need to freeze the VMs if they don't need the help of the virtualization stack. Traditional VM-PHU, we get the update going, the virtual machines get suspended, the update finishes, the reboots of the virtualization stack or some other host OS components that they depend on get updated, and then we resume those virtual machines, restoring their state, but with virtual processor auto-suspend and virtual function keep-alive, what we can do is, for a virtual machine that is just doing its normal thing of doing storage and network I/Os, is that they don't have to stop at all. The only time that you need to freeze a virtual machine is if that virtual machine needs the help of the virtualization stack, so it's calling out to the host in the virtualization stack. At that point, what we do is freeze the virtual machine, pin that request, and then when the updates finish, we can resume that virtual machine. For the most part, the vast majority of virtual machines now through virtual machine-preserving host update will see zero impact, and I've got a demo of those two compared so you can see side by side. What you're seeing here on the top left is the host underneath the virtual machine, pinging the virtual machine. On the bottom left, that's a window command line on the virtual machine, which we've got an app that's just going to be counting, and on the right side, you can see that is where we're going to be performing these management operations on the host virtualization stack. The bottom is where we see Hyper-V manager, which I'm sure a lot of you are familiar with. So what we're going to do now as this virtual machine is running, we're going to do a fast

save, which is a standard virtual, VM-PHU update. Now it's fast saved. You can see that when the fast save happened, everything froze on the left. Now we can update the virtualization stack, and then we resume. Of course, this would be completely automated, and so that outage would be very minimal, but here, I did it manually so you could see actually what happens in expanded timeline. Now what we're going to do is move to a machine that has these updates that I talked about, the keep-alive and the virtual processor auto-suspend, so now we've got the same thing, same configuration, but now when we fast save the virtual machines, their state changes in the bottom right, but you can see the pings and the counting are uninterrupted. That virtual machine is still running, even though it has no virtualization stack there to support it, because it doesn't need it. At this point, we're restarting the whole virtualization stack, so it's completely shut down, and now we're starting it up again, and then we fast restore the virtual machine, and through that whole time, the virtual machine had no idea that was going on. This is really going to help us, again, keep the fleet updated with no impact to your virtual machines.

[ Applause ]

Now, let me talk a little bit about storage requirements in AI workloads, because one of the things that you see is, lots of AI workloads are demanding huge amounts of storage. You hear all about the compute side of ML and AI, not so much about the storage side, but it's a very real optimization problem at scale, especially when you got large AI models that you're training, for example. In the training, especially now when you have videos, because these models are multi-modal, now you're starting to talk about not terabytes or hundreds of terabytes of data. Now you're talking about petabytes, and in some cases, even up to hundreds of petabytes of data, especially for autonomous driving model training. So huge amounts of data you need to get into the AI infrastructure and to the GPUs. Second, as those models are running, they need to checkpoint, so if there's a failure, they can go back to a previous checkpoint, and those can be terabytes in size as well, and for inference or when the model is being used after it's been trained, again, when we have an infrastructure at scale and a new model we're rolling out, that model can actually be terabytes in size, and we need to deploy it to thousands, tens of thousands of machines, and deploy them onto those GPUs. We need to do that very efficiently, and if a server fails, it needs to get access to those GPU models as well. A typical server will have many GPU models on it so that it has them cached there so that it can run them, so we want to make sure those caches are hot. Lots of cases, on both sides, where having very high bandwidth storage can really optimize the infrastructure. If you've used Azure Blob stores, with Block Blob, you know that you can get tens of gigabytes per second of throughput typically, both reading and writing, but what if I could tell you that, driven in a large part by OpenAI and the scale of the training they're doing, we've been working on building something called a scaled storage account. The scaled storage account is actually a logical storage account that, underneath the hood, is using slices made up of storage accounts spread across our storage infrastructure, and that means that they have access to the network bandwidth across the data center, that

means they have access to storage nodes across the data center, and so we can now get hundreds of petabytes of capacity into one account with terabits per second of throughput, not gigabits per second, but terabits per second, and hundreds of thousands of IOPS. Let's go take a look at that in action, and what I'm going to show you here is, this is a Slurm cluster, which is part of our cycle cloud offering on our HPC platform for deploying a BlobFuse driver that is mounting across 320 servers, there's one of these scaled storage accounts. What you're going to see here is first me dumping the configuration for the BlobFuse driver, which is pointing at this training data folder and connecting it to the Blob Storage account. Now we're going to run the Slurm, this parallel SSH to go and set that BlobFuse configuration across those 320 servers. Now we're going to actually SSH into one of them so you can see the storage account mount right there. BlobFuse 2 pointing at training data, so they're all pointing at the same storage account with similar mounts, and now we're going to run a parallel job using Slurm to have each one of those servers write as fast as they possibly can to that storage account, stop, and then read back, and you can see these dials here on this dashboard, and you're reading that right. That's 15 terabits per second of bandwidth, and now they're going to stop. You're going to see them wind down their reads -- their writes and read back what they wrote. And if 15 wasn't impressive, how about 25 terabits per second of bandwidth? This is available --

[ Applause ]

This is available for customers that need extreme limit performance from their stale storage accounts, which we're doing as kind of a white-glove offering right now. All right, let me switch and talk a little bit about cloud-native innovations, so moving into the compute space and how we describe compute, and I think one of the things that I announced here at Build two years ago, and some of you might have been in the room or watched online, is I announced something called Azure Linux. Does anybody remember that? Yeah -- which, for me, was kind of a wow moment in my professional career, having been at Microsoft since 2006, back when Linux was not necessarily the operating system that Microsoft used a lot. Let me put it that way, and --

[ Laughter ]

In 2014, Satya announced that Microsoft loves Linux, which that was a huge moment. In 2012, we launched with IAS, with Linux, so this journey of, wow, Microsoft using Linux, making Linux available to customers, getting to the point where we had our own Linux distro. Azure Linux was a big milestone for me, but we've been innovating on Linux as well over time, and one of the things that we've been improving on is the ability for Linux to be secure. If you take a Linux code integrity, typical approach to code integrity, maybe not even to have code integrity, and what that means is that if you've got binaries that come down, executables, either to the host OS or to the container OS, into the container, that they can run. What the typical approach, though, if you care about code integrity, which I recommend strongly you do, it's one of the core principles of secure software deployments,

is to have everything code signed with policies for what can run and what can't. By the way, I'm Deputy Chief Information Security Officer for Azure now, so this is burned into my head, to say to everybody all the time, is to make sure that only the approved binaries can run, and SELinux is one way that people use to set a list of what binaries can run, but that doesn't solve the container problem. How do you stop, through the host policy, things from running in the container? What we've been working on is something called LinuxGuard, and with LinuxGuard, you use DM-verity with policies for container image layers that specify what signatures are allowed for things to run, either in the host operating system or in the containers that run on top of that container host. What I'm going to do is show you here, underneath the hood, you have your standard container layers that you see there on the right. Your container image registry has signatures for each layer, and then you've got this policy running on an immutable op -- Linux operating system, with SELinux for security controls and DM-verity and something called integrity policy enforcement, which is where that policy for what can run and what can't gets enforced. And then you've got the containers there that come with their signatures, that are verified against the IP policy. Let's go see LinuxGuard in action. So at the top, I've got a container host. At the bottom, I've got actually what we'll be seeing inside of a container, and so I'm going to just Wget this executable, "hello host", and I was able to run "Wget hello container." Container emit -- container executable, able to run that inside the container, and now here is our container image that we're going to use with LinuxGuard, which has those signatures as part of its artifacts, as well as, here are the manifests for it, which has the layers, which point at those signatures. Now, with the policy set up to only allow those signed executables to run, you can see even on the host, I can't run "hello-host," and if I apply that container, alpine container that I made, wget that, and then -- or I'm in the container now. I wget that "hello container", that won't run either, and I also get a log of exactly what was allowed to run and what wasn't there. You can see "action deny" on those two images because of that IPE policy that is enforced by this combination of configuration that we call LinuxGuard. We already upstreamed IPE, and that's part of standard Linux distros now. We are working on upstreaming the components of LinuxGuard as well. Again, Microsoft releasing in its own version of Linux. Two years later, here I am at Build, talking about Microsoft upstreaming security enhancements to Linux, which is, again, another cool milestone for me.

[ Applause ]

If you were at Build two years ago, you saw me announce something that we were working on, that was really cool, and I'm here to give you an update on that, and it has to do with how we isolate hostile multi-tenant code in our production systems. In fact, we came up with this term internally at Microsoft, hostile multi-tenancy, which puts us in a frame of mind to think about how we should be looking at customer code that runs on our cloud, meaning, it's not necessarily hostile, but you have to assume it's hostile. It wants to do bad things, and that means we need to isolate it from the infrastructure, we need to isolate it securely from other customers, and the strongest security boundary we have developed at

Microsoft is the virtualization security boundary with Hyper-V. That is our only approved hostile multi-tenant security isolation boundary, and I own the policy now because I'm the Deputy CISO for Azure, so trust me, that's the policy. We have virtual machines, you've got containers, and we introduced Hyper-V isolated containers for both Linux. We call it LCOW and WCOW Hyper-V isolated containers, and it's Linux containers on Windows, Windows containers on Windows, WCOW. That's where that comes from, and then I announced two years ago something for WebAssembly because there's cases where we want something even smaller than a container. We want something that can execute in-line or inside of a storage stack, and those are typically called user-defined functions. User-defined functions, very lightweight piece of code that performs some transform on some piece of data, and we don't have to launch a whole virtual machine. We just want a tiny piece of code to run, but we need to sandbox it, again, with our only approved isolation technology, Hyper-V sandboxes. WebAssembly is a great runtime for a user to find functions. It supports a bunch of different languages, it's got standard interfaces for talking to the outside world. Microsoft's been a huge contributor to WASI and WASM, and some of you know about Blazor, for example. It's just one example of .NET inside of WASM. Here, we've put WebAssembly into a micro sandbox, and this is our internally approved platform for running micro virtual machines for user-defined functions, and this is called Hyperlight, the technology that we built to do this, and what Hyperlight uses is hypervisor APIs, standard hypervisor APIs, to create a micro VM, and these micro VMs can literally be tens of megabytes in size, not hundreds of megabytes or gigabytes like typical virtualization sandboxes, but tens of megabytes, just big enough to run a tiny piece of code. The application then talks to the Hyperlight API, which talks to the Hyperlight guest API to set up the user-defined function inside that micro sandbox, and then once that's set up, the application can talk directly to that code inside that sandbox. So, one of the things, here is the update, one of the updates, is that we're now productizing this inside the Azure infrastructure, inside of Azure Front Door, AFD. AFD is our front-door serving plane that runs across PoPs all over the world, does things like denial-of-service protection, does things like SSL termination and -- or TLS termination, and does things like security, and one of the things you'll be able to do is specify user-defined functions that execute right on the edge, inside of AFD, and the way we can do this efficiently is -- and securely, is by isolating your user-defined function inside of a micro VM and saying that you've deployed these things as WASM code. Let's go take a look at that in action. And now here on the left side, I've got my user-defined function. If you take a close look at this, you'll see I'm a little embarrassed by that code on the left. Does anybody know why I'm embarrassed by the code on the left? There's a big flaw in it for new code. It's written in C. SPEAKER 1: It's not rust-based.

Mark Russinovich: Yep, somebody said it.

[ Laughter ]

It's not Rust-based, so I did not write this code. When I found out about this code, this part of the demo, it was too late to change it, but -- so I apologize for it being C -- new C code. New code that's system code. They can't tell their garbage collection should be written in... Rust-- yes, very good. Let's try that again. Should be written in...

Audience: Rust. MARK RUSSINOVICH: Yes. As Deputy Chief Information Security Officer for Azure, that is the policy in Azure. In any case, we've got a user-defined function, there's a crop face function, a crop image function, and on the right side, we've got, you'll see here in a second, our AFD runtime, which takes a user-defined function and executes it in this micro sandbox. It creates the Hyperlight sandbox and uses sandbox builder API to call the guest function, crop face function, inside that sandbox, and then gets the response from an HTTP request into that sandbox. So here, I've got a demo where I'm going to upload a picture of myself from 2009 when I was in Windows. Back then, serverless was a real big problem. Think about that for a second. Okay, never mind, so then we've got -- you can see that what that crop image is being executed and it's trimming that, and it's a fraction of the size, and that happened all inside that edge function, on the edge before it even hit the website, so -- by the way, just let me explain that serverless was a problem in 2009. Serverless now is really cool, and I'm going to talk about it in a second, but in 2009, if you were serverless, the only thing that meant is that you had no servers.

[ Laughter ]

So -- all right. I need to work on the delivery there, so...

[ Laughter ]

Now, another announcement I've got is that Hyperlight is being contributed to CNCF. So, this is completely open source, under over governance and it not only supports Hyper-V, but it also supports KVM. So, this is something you can take and regardless of the hypervisor infrastructure you're using and use, you can create these micro sandboxes yourself. So, it will be cool to see what you do with that.

[ Applause ]

So, speaking of serverless, how many people have heard of Azure Container Instances or ACI? Has anybody used ACI? So, ACI, just, it's a serverless container API and infrastructure that we define in Azure. In fact, we were the first company, hyperscaler cloud to come up with serverless containers with ACI. And then, you saw other cloud providers come up with their own. ACI is our future of our infrastructure. So much of Azure and more and more is being built on top of ACI because we believe in a future that is serverless, where you just focus on containers and applications, not on infrastructure. And let the platform define and implement the orchestration of the application for you. But one of the ways that we made ACI available to customers to use besides a stand alone is as part of AKS, where you can create a virtual node using an ACI instance. And this allows you, a lot of customers burst

out into ACI from their fixed pool of AKS nodes or their server AKS nodes into serverless ACI nodes. But, one of the things we've done, both to support rapid scale out for these cases, where it's for bursting as well as for scaling out, just a purely serverless application, is we defined end groups, which are like virtual machine scale sets for containers. But we've also, just like we've got for virtual machines, defined standby pools. And this allows customers for Microsoft-owned services, or customers like you, to come and say, "I want a standby pool of this image, so, that if I do need to burst out, I get extremely low latency." And so, these standby pools, which cost a fraction of what just having these containers running as part of your normal provision capacity would be, allow this scale. And what I'm going to show you is a dramatic demo and this is something that I think is really cool. You can see this demo as launching lots of containers. Now, what's lots of containers? 100? Is that a lot of containers? 1,000? 2,000? Let's go see. So, here on the left, I'm going to run my container launching script, it's going to launch ACI containers. On the right side, I'm just going to take a time of day, so, that we can see how long it takes to launch all these containers. Now, I'm going to paste in my launch and this is going to kubectl deploy three deployments each of, you can see, 2,500 containers. So, for a total of 10,000 containers. And... at the end of the day, you can come back to this room and see what-- no, I'm just kidding.

[ Laughter ]

What you're going to see here is these containers spinning up. Now, you're going to see they're, kind of, slow to start. You can see the number ready there on the left side, for each deployment and the number available total there on the right side. And here we go, we're off to the races. And I thought this was just impressive enough that we should just look at it in awe.

[ Laughter ]

Because 10,000 containers, launching off of ACI standby pools and these are full blown containers with operating systems and apps in them, not just empty containers. And here we go, coming up to the finish line here, 24,000s we see and there we go. And... we're finished basically, for them to come up. This command script is going to finish and we can take a look at the total count for each deployment. And you can see that comes up to 10,000 and we can just take a look at the time and we're not even being that precise because just the fact that it's under two minutes is pretty mind blowing.

[ Applause ]

Alright, so, let me switch into higher level construct here and talk a little bit about a team under me called Azure Incubations. Azure Incubations, I started in 2016, it was, kind of, an accidental starting of an incubations team under me. Because I had some technical assistance that would help me with demos, research, presentations that were really creative. And as we talked about, "Hey, what do we need in cloud native computing?" They ac

that one of them did, Yaron Schneider, which he's here at Build actually, he's at a start up now, but he's here at Build. He actually created something called KEDA, the Kubernetes Event-Driven Autoscaler. So, that was our first incubation and then we launched that. It graduated into CNCF in August, 2023. One of the principals is open source, CNCF, open governance. Then, another incubation, this is actually an interesting one. Copa, Project Copa, Copacetics, has anybody heard of this? So, if you're into security, you're going to hear about it because this is the way that you patch container images. In the same way that you can pass traditional software without having to re-build everything. We said, "Hey, we need to be able to patch, not re-build." Because re-building means precious time and waiting, especially with containers, we've got layers. Let's go and just patch. That is now a sandbox project, it should be going into incubating really soon, September, 2023. We at Microsoft use this on our container registries and we patch literally millions of container images every single month using Copacetic. Which does us a security scan of an image, figures out what patches apply. And then, I guess, those patches applied are additional layers that go into the container image without requiring us to re-build. And then, Dapr, how many of you have heard of Dapr? So, quite a few of you, that's awesome to see. It came out of the Incubations team. This one, I'm really proud, graduated, CNCF in November of 2024, exactly five years after we announced it. But we've been working on some -- and here's the announcement. But we've been working on some new incubations. One of them is called Project Radius and this one is a sandbox project from last year. We've been working on this diligently, we were working with several customers that are taking it to production now. So, we think this is on a good path to eventual graduation as well and I think really a game changer, especially for companies that believe in platform engineering. I'll talk about that in a second and then, we have another one, which is really, really cool for change driven architectures. So, let's take a closer look at Radius and then, Drasi. So, one of the things, if you're building a cloud in an application, it's more than just kubernetes. As you know, you've got to deal with all of your services that your application uses besides your containers. You've got to deal with the networking setup, you've got to deal with multiple clouds if you're multi-cloud. Including on prem, if you're hybrid. You've got to deal with Helm charts, you've got to deal with bash scripts. You've got to deal with sometimes parasol scripts, I've seen everything for complex applications. Consisting of all, sorts of, technology, in terraform recipes, everything just melded together, lots of complexity. How can we simplify things? That's the goal of Radius and so, the goals of Radius are make it so that a team can collaborate on the app. Not have to learn the infra in the process, just focus on the application's architecture and its requirements. Make it so that you combine the app to arbitrary infrastructures. Which either, if you're doing dev ops, you can do yourself or a platform engineering team can go take an app and say, "I'm deploying this into production over here. I'm deploying this over in the stage environment over here." Different infrastructures, "I'm deploying it in this cloud. I'm deploying it to that cloud. I'm deploying it to on prem." You get a graph, that shows you the relationships of your containers, your code, to the resources they need connections to. Where those connections, when you specify them, help you. Like by provisioning secrets or by making network connections

open. So, this approach encourages you to leverage Radius because it's going to help you and at the same time it's helping you, you now get a visualization of exactly the dependencies in your app. And then, of course, like other incubation projects, it's cloud neutral. So, just to give you a high level of how this looks before I show you a quick demo. An application you can think of, in the way that you would imagine an application. A front end and a back end, talking to a cache and a back end database. And you say, "I want to use Redis API, I want to use MongoDB as the API for the back end. I want this thing to be independent of infrastructure, though." So, with Radius, you bind through something called recipes. When you deploy to Radius, this app, it says, "Oh, you want MongoDB, the recipe to deploy MongoDB is this, it's stand up a MongoDB container." Same thing for Redis, so, this would be the recipes for deploying onto some kubernetes cluster with local instances of those services. But if you're deploying into a cloud platform, when you want to use managed versions of those, you just simply specify recipes that point into a landing zone for those managed services. So, Azure Cache for Redis, or Azure CosmosDB for the Mongo support. And then, for AWS, you specify those resources. The app didn't need to change at all, just the recipes and the environment for the landing zone that a platform engineering team, like I said, could be responsible for. So, one of the things that we've done is not just support kubernetes. Like I said, our vision is serverless. You don't have to worry about servers, you don't have to worry about nodes and kubernetes clusters. You just worry about the app and our infrastructure for serverless, their primitive is ACIN groups. So, we've been working on making it so that you can take in a very composable way, target different compute hosting platforms for Radius. Either kubernetes, which is the one that we did first and now, we're working on ACIN groups. And so, here you can see an application that I'm going to show you, a demo of deploying called TraderX. TraderX is a reference application, so, we didn't make it up, just we wanted a real media application. It's been created by FINOS, which is a financial operating system foundation, consortium of the banks, financial institutions got together and said, "We want to standardize things. And for cloud computing, let's have a reference application." They created TraderX, so, it consists of seven or eight micro services, like a real trading application. And we just took that and we said, "Let's ratify it." That's what you call it, ratifying it. And so, we had a product manager on our team, ratify the FINOS app. You know, which would consist of Helm charts and bash scripts. You know how long it took him to ratify it? One day. So, he took this micro service, real application, ratified it in one day. Now that it was ratified, we could easily create recipes to deploy to either AKS or to ACIN groups. So, that's what I'm about to show you. Here's the architectural diagram of FINOS from the FINOS documentation site. You can see a bunch of blocks there, representing the different micro services. The trading service, the trade logging service. And... oops. Let's get that demo to play. So, here you can see the application Bicep manifest for the TraderX application. This is what an architect would develop. And then, we've got two manifests for recipes for the landing zones. The first one here deploys, sets up the environment for kubernetes. And the second one for ACI. Now, we're going to deploy it to both of those and you can see the W targets the different recipe sets, there were, kind of, templates for those recipes. And on the right side for AKS, so,

we're deploying now the FINOS application to both serverless environment as well as to a kubernetes cluster. And in the ACI case, there's no kubernetes, it's just ACIN groups. And you can see the resources there, all with those connections. This is the application graph I was talking about. And on the right side, well here's the graph that you can see right there. Front end, talking to resources, connections to the database and the trade feed. And then, if we go to the Azure portal, to actually see what those recipes did and created. They created a bunch of ACI container instances, they created ingress controllers for them as well. And so, this is what happened underneath the hood with those recipes. Which are a Bicep manifest or they can be terraform recipes as well. So, leverage your existing investments and here, this website just shows you that that FINOS app is really up and running now, on top of Acin groups. So, this is, if you were interested, this is completely open, so, you can go, check out. There is a URL there that you saw and you can go play with the FINOS app easily. And feel free to contribute, feel free to reach out if your company is interested in working with us on this and taking, you know, interest in, "Hey, maybe this might be something we want to take to production." Because like I said, we already have one major customer in production with this as an incubation. And we've got a few others that are imminently going to go to production. It's, kind of, the same graduation, the same journey that we took with Dapr. And I believe that the combination of Dapr plus Radius gives you insight into the way your application's architected. Gives you monitoring support, gives you best practices and gives you separation from infrastructure and cloud portability, if that's interesting to you. So, a lot of benefits come from the combination of these technologies. Which you can use individually or together. Now, let's go take a look at Drasi. How many of you have had to write an application where there's data that's changing and your application logic needs to say, if this happens and that happens, but this didn't happen, I need to fire an event or put something up in a UX or write something out to the database. So, a lot of you have and that means that you probably have battle scars from having to write that logic. Because it's really complicated because there's so many ways that data updates are available to you, depending on what you're trying to get state changes from. And in some cases, the worst case, it's multiple of these, not just pulling from a database that doesn't support a change feed where you're saying every ten seconds, "Hey, did this variable change, did this row, element in this row change? Did this one change, did these change to these above grade in this value?" But change feeds, another one where they get fed to you and that's great for getting the data to you. But you still have to keep track of the states you're interested in and that's the complex logic. And then, streaming applications, too, take care of getting it to you and in worse cases, that you've got multiple of these and you're like, "Did this change over here and is that above this value over here? And oh now it's interesting, now I need to do something. Oh, but I need to remember I already did that." Just this huge amount of complexity that we're aiming to just make go away. And the way we're doing that with Drasi is we've got this, kind of, continuous query pattern. Those changes, those state changes you're interested in, you simply define as right now it's a Cypher and it's going to be GraphQL soon. Query, database query, it's effectively a database query. We're saying when this happens, return this. And that's your standing

query, you give that to Drasi, you wire up the sources, which can be one or more of these, and you specify what you want to have happen when that query is satisfied. And-- or the query's results change because it will also let you know when the query results set changes, where some elements come fall out of its query results set, or come into the query results set, you want to know about that. And you can see here now, if we want to update the logic, it's simply changing that query. Everything else remains the same and I don't need to worry about whether it's streaming, whether it's change feeds, whether polling is underneath. That's what the sources worry about, which are canned, you just wire them up. So let's see a demo of that in action and what I've got here is a curbside pick up application. It shows the power of Drasi to do joins across multiple data sources with queries, continuous query that's, kind of, intuitive. For curbside pickup, you want to know a, is the order ready? And b, is the car at the curbside? And then, you want to have somebody run out with the order or a robot do it, I guess. So, that's the, kind of, standing query that you want. So, how do you do that when you've got two databases? The one on the left, that is where I'm going to have, in Postgres, I'm going to have whether the orders are ready or not. And the database on the right side is the database that's keeping track of whether cars are at the curbside or not. And I need to do joins across them and say-- you'll see here with my query. So, there's orders and their status and my SQL database, the cars. And you can see preparing and parking statuses and here's my reaction, which is a single R, which is going to go to the website, to show us an update. And here's my continuous query-- match, orders, pick up by vehicles, where the status of the order is ready and the vehicle location is curbside. And so now we've got our query, we've submitted that to Drasi and now, we're going to change manually the status of one of those orders to ready and you saw nothing happened in the Drasi window, in the app window. Now, we change to curbside and instantly, Drasi satisfied that continuous query and we saw the signal R fire to update that UX. But Drasi can also, we've been augmenting it with additional capabilities like, what happens if you want to know when nothing's happened? Like a car's been at the curbside for more than ten minutes and it's waiting and we tell them something. So, here's a non event, that we set with a continuous query. When the time, nothing's changed in ten seconds, so, we say this car is at curbside. The order's not ready and now, in a few seconds, you're going to see Drasi go, "Oh, there's a result now in this query and it's that car that's been waiting for ten seconds." Imagine now going and writing this logic across these two databases yourself, for both of these cases. And then going, "Wait a minute, you know, I also want to know if it's this and that. Is it a premium customer or not? We need to flag that, too." And, you know, you can imagine how the logic on this can get even more complicated. And with Drasi, it's simply going and modifying the continuous query with additional conditions and that's it. So, this open source, published in GitHub, with its own website you can go to. Download the code, play with it in a container and make contributions. So, this, I think is the future of reactive programming right here. Open source as well. Alright, now, yeah--

[ Applause ]

Now let me talk a little bit about confidential computing, which is the future of computing and security and computing. And for those of you not familiar with confidential computing, and I assume that a lot of you probably by now, when I asked this question five years ago, very few hands would go up. I assume a lot more would go up now. And that is when you talk about protecting data through its life cycle, everybody's familiar with protecting data while it's in transit with TLS. Everybody's familiar with protecting what's at rest with encryption at rest. What's been missing is protecting it while it's in use, where the data's being processed, it's sitting there out in the open. And that means the hypervisor can get to it, the host operating system can get to it. Operators accessing the machine can get to it. If it's in an app, the app operators can get to it. The code that gets into that container or virtual machine can get to it. So, exposed to lots of threats. We want to protect it against those threats, so, hardware companies have been working on something with our partnership over the last decade or so, to create what are called Trusted Execution Environments or TEEs, that do two things. One is they create, basically a box, a shield around that code and data. And they also provide a cryptographic hardware based quote or attestation of what is inside that box. And that allows somebody else to decide whether they trust the code in that box to release it a key, that will allow it to decrypt its data. And that trust then allows that data to be decrypted in the container and then protected from outside threats. And we've been at this for ten years now, over ten years. And you can see some of the key milestones on this journey, including the introduction, with our partnership with NVIDIA bringing confidential GPUs. Intel TDX confidential virtual machines, AMD confidential virtual machines. We actually, at Microsoft came up with the term "confidential computing." If you go back to that 2015 paper, that's the first time that term appears. And we've got, over this last ten years, built out lots of confidential enabled services. Confidential data breaks, confidential AKS, confidential AVD, confidential managed HSM. Confidential Kusto, like the list goes on and on of the confidential capabilities we have on top of the hardware capabilities. But one of the exciting things is the rise of confidential GPUs. And I apologize, I was like, wouldn't it be cool if after all the AI that you've been inundated with here, if there was one session that didn't have any AI in it?

[ Laughter ]

And I'm sorry that there is a little bit of AI in this one, but almost none, so, that, you know, a little break for you. So, in this one, it's the confidential GPUs, which allow you to use confidential computing to protect model IP, to protect the data that goes into the training and the fine tuning of the model. And to protect multi party scenarios as well and what I'm going to show you is just one of the things that we're just bringing out right now. We introduced confidential GPUs a while ago, so, this is something that we already have in market. So, you can go use confidential. H100 GPUs and Azure, that supports single GPU models, so, small models that fit on a single GPU. Like confidential whisper. But we've introduced multi GPU models and this has allowed companies like ServiceNow to build agentic flows on top of confidential computing to protect the privacy of their data. And

here's where sellers are sending a request about commissions and all of that RAG data is protected with confidential computing and the quote's protected with confidential computing. That those quotes are made available to agents, that can then do processing on top of it, but protecting the privacy of the sellers. And that is built on top of NVLink with confidential GPUs from NVIDIA, which they've introduced now. And we've got support for and the H200 GPUs that we're working on bringing out. And here, the CPU and GPUs are connected through confidential computing. And the GPUs are also connected together in a confidential network. So, I'm going to show you here and this is not yet available, but we've got it here working in our preview cluster. It's a standard ND v5, we're going to say it's confidential. This has eight H200 GPUs on it. We're going to take a look at some of the meta data in that virtual machine. We're going to actually get an attestation for the virtual machine itself, which says that it's running in TDX, Intel TDX confidential virtual machine. But now we want to set up the GPUs with a model, in this we're going to be using DeepSeek-R1. We're setting those up in a confidential way and we're also going to get an attestation, that we can then present to somebody that shows that they are wired up confidentially using something called protected PCIE that we worked with NVIDIA on, to support multi GPUs over NVLink. And so, it's getting attestation for each of the eight GPUs and we'll roll that up into a summary, so, everything passed there. And now, what we can do, there you can see the models, DeepSeek-R1. And what we're going to do is go do an inference request and see this is completely confidential, encrypted just so that virtual machine and the GPU can see the prompt. It's processed there, inside the confidential boundary, encrypted to the receiver and now, that receiver can decrypt it like you just saw happen with nothing else having access. The host operator can't see it, the hypervisor can't see it, Microsoft operators can't see it. Even with physical access, you'd have a very tough time getting access to this data, so, that is again, the future of confidential computing.

[ Applause ]

Alright, so, I was going to talk about the future, something future in computing that's really out there and that is optical computing. So, our Microsoft research lab in Cambridge, which has an optics lab, has been thinking, "What if we could use free space optics to do compute? " It would be so environmentally friendly because you could do it at room temperature and it's also at, pun intended, light speed.

[ Laughter ]

So, the thing is, with light, you have some tools to be able to do basic operations. Like for multiplication, you can put a filter, that's effectively a multiply operation. And for addition, you can combine light onto a sensor and that, to create a pixel and that is an example of addition. And if you take these operations and map them into neural networks, then you can, if you're controlling optics the right way, with light going through those optics, do those addition and multiplication operations, to do AI. And so, this is the world's first analogue optical computer, thought I'd bring one here from Cambridge, for you to take a look at.

[ Applause ]

And I'm going to press this button here, you can see an image of this here in a second. And if you come back here and take a look, this is what comes out of the optical computer. My finger's over it, oh yes, out of the optical computer there, that's the light beam. You've got the inputs to the optical computer and then you've got, well, these are the weights actually to the optical computer. The positive and negative weights that go in, to tune the configuration of the neural network. And then this is the data input to the optical computer, this is using micro LEDs, thank you. If we go back to the slides, then you can see this is SIMA sensors, the projectors that you saw and the micro LED. And so, let me show you, this is a recording of a demo from the MSR Cambridge lab for digit classification with a model that's just a few thousand parameters. So, it's a tiny, tiny model, but this is how research goes. You start with something and then figure out how to work and then you refine it and scale it. And so, here we are with a digit classification and I thought it would be appropriate to do, can anybody guess what the next number's going to be? Yes, that's good, here we go. And there's the recognition, so, it's a little slow, so, they've got some work to do. But you can see here's the zero and it's going to recognize this now. Passing that through the optical system and there's the zero. So, you just saw some of the first neural network operations being done completely with light in the world.

[ Applause ]

Alright, one closing demo that I've got to show you, just for fun. I had to put this in, it's not really the future of computing, it's really the current version of computing. And there's some AI here because I've vibe coded this. So, some of you have seen Task Managers on monster machines and this is their current mega Godzilla beast, virtual machine. This virtual machine has 1,792 virtual processors.

[ Laughter ]

And it has... 32 terabytes of RAM. So, what can you do with a machine like that? You can do this.

[ Laughter ] [ Applause ]

So, this cost about 10K, so, I've got a GoFundMe set up, so, feel--

[ Laughter ]

Alright, well that brings me to the conclusion of the talk and I hope you found this interesting, got a little taste of things that we're doing across the whole platform. And I hope you enjoy a great Build.

[ Applause ] [ Music ]

END